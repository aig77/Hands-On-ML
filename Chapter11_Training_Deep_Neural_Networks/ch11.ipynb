{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training Deep Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation works by going from the output layer to the input layer, propagating the error gradient along the way. After computing the gradient of the cost function with regard to each parameter in the network, it uses the gradients to update each parameter with a Gradient Descent step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients often get smaller as the algorithm progresses down to lower layers. As a result, the Gradient Descent update leabes the lower layers' connection weights virtually unchanged, and training never converges to a good solution. This is the *vanishing gradients* problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the opposite can happen: the gradients can grow bigger until layers get insanely large weight updates and the algorithm diverges. This is the *exploding gradients* problem (recurrent neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at logistic activation function, when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is nothing left for the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorot and Bengio proposed that the signal needs to flow properly in both directions: in the foward direction when making predictions, and the reverse direction when backpropagating gradients. For the signal to flow properly, they argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing througha layer in the reverse direction. It's not possible to gurantee both, so they proposed a compromise. Connection weights of each layer must be initialized randomly using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$fan_{avg} = (fan_{in} + fan_{out}) / 2$\n",
    "\n",
    "|Initialization|Activation Functions|$\\sigma^{2}$ (Normal)|\n",
    "|--------------|:-------------------|:-------------------|\n",
    "|Glorot |None, tanh, logistic, softmax| $1/fan_{avg}$|\n",
    "|He|ReLU and variants| $2/fan_{in}$|\n",
    "|LeCunn|SELU|$1/fan_{in}$|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing $fan_{avg}$ with $fan_{in}$ yields LeCunn initialization. LeCunn and Glorot initialization are equivalent when $fan_{in}$ = $fan_{out}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Keras uses Glorot with a uniform distribution. Can be changed to He initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For He initialization with uniform dist but based on $fan_{avg}$ rather than $fan_{in}$, use `VarianceScaling`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_avg_init = keras.initializer.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other activation functions (besides sigmoid) work better on Deep Networks. Especially ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU isn't perfect though. It suffers from *dying ReLUs*: during training, some neurons \"die\" meaning they stop outputting anything other than 0. A neuron dies when its weights are tweaked in a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it keeps outputting zeros, and Gradient Descent does not affect it anymore beacuse the gradient of ReLU is zero when its input is negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LeakyReLU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this, use *Leaky ReLU*. Ensures that neurons never die. Defined as:<br>\n",
    "$LeakyReLU_{\\alpha}(z) = max(\\alpha z, z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ defines how much the function \"leaks\": it's the slope of the function for z<0 and is typically set to 0.01. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PReLU also outperforms ReLU in many cases. $\\alpha$ is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outperforms all ReLU variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![elu_formula](elu_form.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![elu](ELU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main drawback is that it is slower to compute than ReLU (due to the use of exponential function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SELU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled variant of ELU. Authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will *self-normalize*: the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, it outperforms other activations. However, some conditions must be met for self-normalization to happen:\n",
    "- Input features must be standardized ($\\mu$=0, $\\sigma$=1)\n",
    "- Every hidden layer's weights must be initialized with LeCunn normal initialization. In Keras, `kernel_initializer=\"lecun_normal\"`\n",
    "- Network architecture must be sequential\n",
    "- All layers are dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use leaky ReLU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    [...]\n",
    "    keras.layer.Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PReLU, replace `LeakyReLU(alpha=0.2)` with `PReLU()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SELU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly reduces possibility of vanishing/exploding gradients. Consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. I.e., the operation lets the model learn the optimal scale and mean of each of the layer's inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Led to huge improvement in the ImageNet classification task (large database of images classified into many classes, commonly used to evaluate computer vision systems). Vanishing gradients problem was strongly reduced, to the point they could use saturating activation functions such as tanh and logistic activation function. Networks were also much less sensitive to weight initialization. They were able to use larger learning rates, significantly speeding up the learning process. It also acts like a regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Batch Normalization with Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `BatchNormalization` layer before or after each hidden layer's activation function; optionally add BN layer as the first layer in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> [(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add BN layers before the activation functions, remove activation function from the hidden layers and add them as separate layers after the BN layers. Moreover, since BN layers include one offset parameter per input, you can remove the bias term from previous layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(10, 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update `momentum` hyperparameter for BN. It is used by BN when it updates the exponential moving average. Good values are close to 1; e.g, 0.9, 0.99, 0.999 (you want more 9s for larger datasets and smaller mini-batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`axis` hyperparameter determines which axis should be normalized. Defaults to -1, meaning that it will normalize the last axis (using the means and std computed across the *other* axes). When inpt batch is 2D ([*batch size, features*]), each input feature will be normalized based on the mean and std computed across all the instances in the batch. E.g., first BN layer in the previous code example will independetly normalize (and rescale and shift) 784 input features. If we move the first BN layer before the `Flatten` layer, the input batches will be 3D ([*batch size, height, width*]); therefore, the BN layer will compute 28 means and 28 std (1 per column of pixels, compputed across all instances in the batch and across all rows in the column). If you want to treat each of the 784 pixes independently, set `axis=[1, 2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip gradients during backpropagation so they never exceed some threshold; another technique to mitigate the exploding gradients. Often used in RNN, since BN is tricky to use in RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer will clip every component of the gradient vector to a value between -1.0 and 1.0. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, clip by norm by setting `clipnorm` instead of `clipvalue`. This will clip the whole gradient if its $l_{2}$ norm is greater than the threshold you picked. If gradients explode during training, try both clipping by value and nrom, with different thresholds, see which option performs best on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Transfer Learning*: resuing lower layers of a prexisting (similar) network to accomplish a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more similar the tasks are the more layers you want to reuse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the Fashion MNIST dataset contained only 8 classes (all the classes except shirt and sandal). Someone built and trained a Keras model on that set and got good performance (>90% accuracy). Call this model A. You want to tackle a different task: you have images of sandals and shirts, and you want to train a binary classifier (0=shirt, 1=sandal). Your dataset is quite small (200 labeled images). When you train a new model for this task (model B) with the same architecture as model A, it performs well (97% accuracy). Since it's a similar task to model A, transfer learning might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model('my_model_A.h5')\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) # reuse all the layers except output layer\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note new model shares layers with model A, so training will affect the layers in model A as well. You must clone model A to use its layers. Clone model A's architecture with `clone_model()` then copy its weights (`clone_model()` does not clone weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train `model_B_on_A` for task B. But output layer was initialized randomly, it will make large errors, so there will be large error gradients that may wreck the reused weights. To avoid, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. Set every layer's `trainable` attribute to `False` and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train the model for the first few epochs, then unfreeze the reused layers (requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing layers, reduce the learning rate, once again to avoid damaging the reused weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=[X_valid_B, y_valid_B])\n",
    "\n",
    "for layer in model_B_on_A[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimzer = keras.optimizers.SGD(lr=1e-4) # default lr is 1e-2\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning works best with deep CNN, whcih tend to learn feature detectors that are much more general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can gather unlabeled training data, you can use it to train an unsupervised model, such as an autoencoder or generative adversarial network. Useful when you have a complex task to solve, no similar model you can reuse, and little labeled training data but plenty of unlabeled training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining on an Auxiliary Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have much labeled training data, one option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for you actual task. The first neural network's lower layers will learn feature detectors that will likely be reusable by the second neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Momentum optimization\n",
    "- Nesterov Accelerated Gradient\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam and Nadam optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where Gradient Descent is like taking small steps to find out which direction to go toward, momentum optimization is like rolling a bowling ball down a gentle slope. It'll pick up momentum until it finds a terminal velocity (if there is some friction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cares about what the previous gradients were: at each iteration it subtracts the local gradient from the *momentum vector* **m** and it updates the weights by adding this momentum vector. Gradient is used for acceleration not speed. Algorithm introduces hyperparameter $\\beta$ for friction set between 0 (high friction) and 1 (no friction). Typical value is 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizer.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawback is that it adds another hyperparameter to tune. But 0.9 is usually fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant of momentum optimization. Measures a gradient of the cost function not at the local position, but slightly ahead in the direction of the momentum. Generally faster than momentum optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizer.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scales down the gradient vector along the steepest dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm decays the learning rate, but does so faster for steep dimensions than for dimensions with gentler slopes. This is called an *adaptive learning rate*. It helps the resulting updates more directly toward the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs well for simple quadratic problems, but often stops too early when training neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixes AdaGrad from stopping early by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). Decay rate $\\beta$ is usually 0.9. Again, another hyperparameter to tune but the default value usually works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note `rho` corresponds to $\\beta$. Except on very simple problems, almost always works better than AdaGrad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam and Nadam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adaptive moment estimation* combines momentum optimization and RMSProp: like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and like RMSProp, it keeps track of an exponentially decaying average of past squared gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*AdaMax*: Adam scales down the parameter updates by the $l_{2}$ norm of the time-decayed gradients (recall that the $l_{2}$ norm is the square root of the sum of squares). AdaMax replaces the $l_{2}$ norm with the  $l_{∞}$ norm (fancy way of saying the max)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nadam*: Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Power scheduling\n",
    "- Exponential scheduling\n",
    "- Piecewise constant scheduling\n",
    "- Performance scheduling\n",
    "- 1cycle scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power scheduling. `decay` is inverse of *s* (number of steps it takes to divide the learning rate by one more unit), Keras assumes *c* is equal to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential and piecewise constant scheduling require defining a function that takes the current epoch and returns the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to avoid hardcoding $\\eta_{0}$ and *s*, create a function that returns a configured function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.01**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `LearningRateScheduler` callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule function can optionally take the current learning rate as a second argument. For example, the following schedule function multiplies the previous learning rate by $0.1^{1/20}$ which results in the same exponential decay (except the decay now starts at the beginning of the epoch 0 instead of 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relies on the optimizer's initial learning rate. Make sure to set it appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer and learning rate get saved when saving a model. `epoch` argument of schedule function does not. It gets reset to 0 every time you call `fit()`. Manually set the `fit()` method's `initial_epoch` argument so the `epoch` starts at the right value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For piecewise constant scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performance scheduling, use `ReduceLROnPlateau` callback. E.g., if you pass the following callback to the `fit()` method, it will multiply the learning rate by 0.5 whenever the best validation loss does not improve for five consecutive epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras offers an alternative to implement learning rate scheduling: define the learning rate using one of the schedules available in `keras.optimizers.schedules`, then pass this learning rate to any optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: exponential decay, peformance, and 1cycle schedling can considerably speed up convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already looked at Early Stopping and Batch Normalization (designed to solve unstable gradients problems but also acts like a pretty good regularizer). Let's look at more:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ℓ_{1}$ and $ℓ_{2}$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use $ℓ_{2}$ regularization to constrain a neural network's connection weights, and/or $ℓ_{1}$ regularization if you want a sparse model (with many weights equal to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                            kernel_initializer='he_normal',\n",
    "                            kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`l2()` returns a regularizer that will be called at each step during training to compute the regularization loss. This is then added to the final loss. Use `keras.regularizers.l1()` if you want $ℓ_{1}$ regularization; for both use `keras.regularizers.l1_l2()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeating the same initialization strategy in all hidden layer, use loops or Python's `functools.partial()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                            activation='elu',\n",
    "                            kernel_initializer='he_normal',\n",
    "                            kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax',\n",
    "                        kernel_initializer='glorot_uniform')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dropout* is one of the most popular regularization techniques for deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every training setp, every neuron (including the input neurons, but always excluding the output neurons) has a probability *p* of being temporarily \"dropped out\", meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyperparameter *p* is called the *dropout rate*, and is typically set between 10% and 50%: closer to 20-30% in RNNs, and closer to 40-50% in CNNs. After training, neurons don't get dropped anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout essentially creates a new, smaller neural network at each training step. The resulting neural network can be seen as an averaging ensemble of all these smaller networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One caveat, we must multiply each input connection weight by the *keep probability* (1-*p*) after training. Alternatively, we can divide each neuron's output by the keep probability during training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is overfitting, you can increase the dropout rate. Conversely, you should decrease it if the model underfits the training set. It can also help to increase the dropout rate for large layers, and reduce it for small ones. Moreover, many architectures only use dropout after the last hidden layer, so you may want to try this if full dropout is too strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo (MC) Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                    for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model(X)` is similar to `model.predict(X)` except it returns a tensor rather than a NumPy array, and it supports the `training` argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make 100 predictions over the test set, and we stack them. Each call to the model returns a matrix with one row per instance and one column per class. Because there are 10,000 instances in the test set and 10 classes, this is a matrix of shape [10000, 10]. We stack 100 such matrices, so `y_probas` is an array of [100, 10000, 10]. Once we average over the first dimension (axis=0) we get `y_proba`, an array of shape [10000, 10], like we would get with a single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model contains other layers that behave in a special way during training (such as `BatchNormalization`), then you should not force training mode like we just did. Instead, replace the `Dropout` layers with the following `MCDropout` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Norm Regluarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each neuron, it constrains the weights **w** of the incoming connections such that ||**w**||$_{2}$ ≤ *r*, where *r* is the max-norm hyperparameter and ||.||$_{2}$ is the $ℓ_{2}$ norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not add a regularization loss term to the overall loss function. Instead, it computes ||**w**||$_{2}$ after each training step and rescaling **w** if needed <br> \n",
    "(**w** <- **w** *r*/||**w**||$_{2}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing *r* increases the amount of regularization and helps reduce overfitting. Can also help alleviate unstable gradients problem (if you are not already using Batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', \n",
    "                    kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Practical Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default DNN configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Hyperparameter        |Default value                              |\n",
    "|:---------------------|:------------------------------------------|\n",
    "|Kernel initializer    |He initialization                          |\n",
    "|Activation function   |ELU                                        |\n",
    "|Normalization         |None if shallow; Batch norm if deep        |\n",
    "|Regularization        |Early stopping (+$ℓ_{2}$ reg if needed)    |\n",
    "|Optimizer             |Momentum optimization (or RMSProp or Nadam)|\n",
    "|learning rate schedule|1cycle                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If network is simple stack of dense layers, use the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN configuration for self-normalizing net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Hyperparameter        |Default value                              |\n",
    "|:---------------------|:------------------------------------------|\n",
    "|Kernel initializer    |LeCun initialization                       |\n",
    "|Activation function   |SELU                                       |\n",
    "|Normalization         |None (self-normalization)                  |\n",
    "|Regularization        |Alpha dropout if needed                    |\n",
    "|Optimizer             |Momentum optimization (or RMSProp or Nadam)|\n",
    "|learning rate schedule|1cycle                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to normalize input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to reuse parts of a pertrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of ulabeled data, or use pretraining on an auxiliary task if you have a lot of labeled data for a similar task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some exceptions:\n",
    "- If you need a sparse model, you can use $ℓ_{1}$ regularization (and optionally zero out the tiny weights after training). If you need an even sparser model, you can use the Tensorflow Model Optimization Toolkit. This will break self-normalization, so you should use the default configuration in this case\n",
    "- If you need a low-latency model (one that performs lightning-fast predictions), you may need to use fewer layers, fold the Batch Normalization layers into the previous layers, and possibly use a faster activation function such as leaky ReLU or just ReLU. Having a sparse model will also help. Finally, you may want to reduce the float precision from 32 bits to 16 or even 8 bits (see \"Deploying a Model to a Mobile or Embedded Device\" on page 685). Again, check out TF-MOT. \n",
    "- If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC Dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
    "<br>\n",
    "No, all the values should be sampled independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Is it OK to initialize the bias terms to 0?**<br>\n",
    "It doesn't make a difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Name three advantages of the SELU activation function over ReLU.**<br>\n",
    "    1. self-normalize (if conditions are right)\n",
    "    2. it can take on negative values (helps solve vanishing/exploding gradients problem)\n",
    "    3. always has a nonzero derivative (avoids dying ReLUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**<br>\n",
    "- SELU: good default\n",
    "- leaky ReLU: faster training\n",
    "- ReLU: simplicity makes it a good alternative default despite it being outperformed by SELU and leaky ReLU\n",
    "- tanh: if you need to output a number between -1 and 1\n",
    "- logistic: if you need to estimate a probability\n",
    "- softmax: in the output layer to output probabilities for mutually exclusive classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **What may happen if you set the `momentum` hyperparameter too close to 1 (e.g., 0.99999) when using an `SGD` optimizer?**<br>\n",
    "Too little friction causes the algorithm to overshoot the global optimum, which will make it oscillate many times before converging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Name three ways you can produce a sparse model.**<br>\n",
    "    1. zero out tiny weights\n",
    "    2. $ℓ_{1}$ regularization\n",
    "    3. TensorFlow Model Optimization Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?**<br>\n",
    "Dropout slows down training (roughly by a factor of 2) but not inference (since it is only active during training). MC dropout slows down training and inference (slightly). You also want to run inference 10 times or more to get better predictions (slowing down inference by a factor of 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Practice training a deep neural network on the CIFAR10 image dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cifar(n_hidden=20, n_neurons=100, learning_rate=1e-3, input_shape=[32,32,3], \n",
    "                activation='elu', kernel_initializer='he_normal', \n",
    "                batch_normalization=False, dropout=False):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "    for _ in range(n_hidden):\n",
    "        if batch_normalization:\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "        if dropout:\n",
    "            model.add(keras.layers.AlphaDropout(rate=0.2))\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=activation, kernel_initializer=kernel_initializer))\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "import math\n",
    "\n",
    "class LRFinder:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "    \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # Log the learning rate\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = logs['loss']\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        # Check whether the loss got too large or NaN\n",
    "        if math.isnan(loss) or loss > self.best_loss * 4:\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "        \n",
    "        # Increase the learning rate for the next batch\n",
    "        lr *= self.lr_mult\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "    \n",
    "    def find(self, X_train, y_train, start_lr, end_lr, batch_size=32, epochs=2):\n",
    "        num_batches = epochs * X_train.shape[0] / batch_size\n",
    "        self.lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n",
    "\n",
    "        # Save weights into a file\n",
    "        self.model.save_weights('tmp.h5')\n",
    "\n",
    "        # Remember the original learning rate\n",
    "        original_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        # Set the initial learning rate\n",
    "        K.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "        callback = keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "        \n",
    "        # Restore the weights to the state before model fitting\n",
    "        self.model.load_weights('tmp.h5')\n",
    "\n",
    "        # Restore original learning rate\n",
    "        K.set_value(self.model.optimizer.lr, original_lr)\n",
    "\n",
    "    def plot_loss(self, xlim=(1e-9,1), ylim=(0,10)):\n",
    "        \"\"\"\n",
    "        Plots the loss.\n",
    "        Parameters:\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "        \"\"\"\n",
    "\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('learning rate (log scale)')\n",
    "        plt.plot(self.lrs, self.losses)\n",
    "        plt.xlim(xlim)\n",
    "        plt.ylim(ylim)\n",
    "        plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with the `keras.datasets.cifar10.load_data()`. The dataset is composed of 60,000 32x32-pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "y_train, y_test = y_train.flatten(), y_test.flatten()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 32, 32, 3), (5000, 32, 32, 3), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000,), (5000,), (10000,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find optimal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "88/88 [==============================] - 4s 26ms/step - loss: 3.5401 - accuracy: 0.1151\n",
      "Epoch 2/5\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 2.9488 - accuracy: 0.1581\n",
      "Epoch 3/5\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 704167680.0000 - accuracy: 0.1010\n",
      "Epoch 4/5\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 3.3170 - accuracy: 0.1002\n",
      "Epoch 5/5\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 104.0750 - accuracy: 0.1026\n"
     ]
    }
   ],
   "source": [
    "base_cifar = build_cifar()\n",
    "lr_finder = LRFinder(base_cifar)\n",
    "lr_finder.find(X_train, y_train, start_lr=1e-6, end_lr=100, batch_size=512, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEOCAYAAACetPCkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkX0lEQVR4nO3deXxddZ3/8dfnJumWtOmW7hvdKLVACy37qlhZRUWQGQVBtIOjDjiMo6O/wZ86Px1/zjjC+MBOHRRxQCxSFQFZRAoWaCEt3Qtd6L6lbdIkzZ7cz/xxT0oIWe6Fe+69J3k/H488mpz7ved+0pPkfb/ne873a+6OiIhIsmLZLkBERKJFwSEiIilRcIiISEoUHCIikhIFh4iIpETBISIiKQktOMysn5m9YmZrzGyDmX2rgzZmZneb2VYzW2tmp4VVj4iIpEd+iPtuAN7v7sfMrABYZmZ/dPflbdpcBkwLPs4EfhL8KyIiOSq0HocnHAu+LAg+2t9teDVwf9B2OTDYzEaHVZOIiLx3oY5xmFmema0GyoBn3H1FuyZjgd1tvt4TbBMRkRwV5qkq3L0FmG1mg4Hfmtksd1/fpol19LT2G8xsAbAAoLCw8PQZM2aEUW5kHKpu4EBVPe8bU0yso/9BkQg4UFXP4eoGZo0tznYpvcLKlSsPu3tJOvYVanC0cvejZrYUuBRoGxx7gPFtvh4H7Ovg+YuARQBz58710tLS8IqNgEUvbOO7T7zOX771IQr7ZuQQiqTd9/64ifte3EHpv1yW7VJ6BTPbma59hXlVVUnQ08DM+gOXAK+3a/YocGNwddVZQKW77w+rpp4iZoluRlwTVEqEub/1syzREubb1dHAL8wsj0RALXb3x8zsVgB3Xwg8AVwObAVqgZtDrKfHsOPBkeVCRN6DlriTp3OtkRRacLj7WmBOB9sXtvncgS+EVUNP1fq7pinxJcri7qjDEU26czyCYupxSA+gU1XRpeCIoNYeh8Y4JMp0qiq6FBwRZBoclx4g7q7LySNKwRFBrd175YZEWdzfehMk0aLgiCCdqpKeIB538hQckaTgiCANjkvUVdc3sWF/JX3y9ScoinTbcQS1vkmLKzkkQuqbWthTUUt5TRPffHQDWw5W8+/XnZrtsuRdUHBEkMY4JEr2V9bx8xd3sLh0N0drmwAo7JPHz26axwXT0zJ1kmSYgiOCYkHvXmMckutW7arglvtepaq+mQ+9byTzZ46ieEABM0YNZHRx/2yXJ++SgiOCNFeVRMGfNh7kCw+uYlRxPx75/DlMLinKdkmSJgqOCNJcVZLrVu4s528fXMVJowbys5vmMayob7ZLkjRScESQ5qqSXLbrSC2fu38lY4r7cd/NZzCksE+2S5I0U3BEkC7HlVzj7izdfIiHS3fz59fL6FeQGPxWaPRMCo4I0g2Akgsam+PsPVrH3oo6Fj6/jWVbDzO8qC/Xnj6eG8+eqDGNHkzBEUGaq0qyray6nmsXvszOI7UADOqXzzevmsmnzppIQZ5u6uvpFBwRpPs4JJtqG5u55b5Syqoa+O5HT2bckP6cOm4wxQMKsl2aZIiCI4J0qkqyobKuiT+/fpD/Wb6LDfsqWXTDXC6ZOTLbZUkWKDgiSIPjkmk7Dtfw8YUvcfhYIyUD+/L9a05RaPRiCo4IMvU4JIMqahq5+b5XaYk7i//mbOZOHEJMC2n0agqOCHprjEPBIeH74q9WsfdoHQ9+9kzmThqa7XIkB+jyhwjSqSrJlMraJl7ceoTPXzhFoSHHKTgiKKZp1SVDth6qBuCUccVZrkRyiYIjgjRXlWTK1rJjAEwdoZv55C0KjgjSXFWSKVsOHqNvfoxxQwZkuxTJIQqOCGq9okU9Dgnb1kPHmFJSRJ6uopI2FBwRpBsAJVO2HDym01TyDgqOCNJcVZIJNQ3N7D1axzQFh7Sj4IggzVUlmfDmoRoApo1UcMjbKTgiSKeqJBO2lCUuxdWpKmlPwRFBugFQMmFr2THyY8bEYYXZLkVyjIIjgjRXlWTClrJjnDC8UOtryDvoJyKCNFeVZMK2Ml1RJR1TcESQTlVJ2Dbuq2LHkRpmjBqU7VIkByk4Iqh1cLxFySEhaGqJ85XfrGFoYV9uPHtitsuRHKRp1SNI93FImP7r+W1s2FfFwk+dxpDCPtkuR3JQaD0OMxtvZs+Z2SYz22Bmt3XQptjM/mBma4I2N4dVT0/SOv2DckPSbe/ROu5+ditXnDKaS2eNznY5kqPC7HE0A3e4+yozGwisNLNn3H1jmzZfADa6+1VmVgK8YWYPuHtjiHVFnu7jkLDc89xWHOcbl5+U7VIkh4XW43D3/e6+Kvi8GtgEjG3fDBhoiXMvRUA5icCRLmhwXMKwp6KWxaW7+cS88YwZ3D/b5UgOy8jguJlNAuYAK9o99GPgJGAfsA64zd3jHTx/gZmVmlnpoUOHwi435+k+DgnDPUu3AfC3F03NciWS60IPDjMrAh4Bbnf3qnYPfwhYDYwBZgM/NrN3XP/n7ovcfa67zy0pKQm54tyn+zgk3Srrmni4dDfXzVVvQ7oXanCYWQGJ0HjA3Zd00ORmYIknbAW2AzPCrKkn0KkqSbdVOytoanGuOEUD4tK9MK+qMuBeYJO7/7CTZruADwTtRwInAm+GVVNPocFxSbfSneXkx4zZ4wdnuxSJgDCvqjoXuAFYZ2arg21fByYAuPtC4DvAfWa2DjDgq+5+OMSaegStOS7p9uqOCt43ZhAD+ujWLuleaD8l7r6MRBh01WYfMD+sGnoqrTku6dTYHGfN7qN86izdJS7J0ZQjEXR8jENdDkmD9fsqaWiOM3fikGyXIhGh4IggDY5LOpXuKAfg9EkKDkmOgiOCLDhqGhyXdCjdUcHEYQMYMbBftkuRiFBwRJDWHJd0cXdW7qxg7sSh2S5FIkTBEUG6HFfSwd357hObOFLTyAXTh2e7HIkQXXsXQRrjkPequSXOvz29mZ/+ZTs3nTOJD586JtslSYQoOCJIc1XJu9XUEufBFbv46V/eZE9FHX91xgS+edXM4/cGiSRDwRFBmqtK3o03DlRzx8OrWb+3irkTh3DnlTP54MyRCg1JmYIjgnSqSlK143ANV/14GQP75vOTT57GZSdrTip59xQcEaTBcUnVktf20tQS5/dfPJdxQwZkuxyJOF1VFUGaq0pS4e48unovZ08eptCQtFBwRFTMNMYhyVm3t5IdR2p15ZSkjYIjomJmOlUlSXl09T4K8ozLZmlcQ9JDwRFRieDIdhWS6+Jx57G1+7lwegnFAwqyXY70EAqOiDLT4Lh0b+P+Kg5U1WtlP0krBUdExcw0V5V069CxBgAmDC3MciXSkyg4IipmWo9DuldV1wRAcX+dppL0UXBElMY4JBkKDgmDgiOiNMYhyahUcEgIFBwRFYuZ7uOQblXWNdG/II8++fpVl/TRT1NE6VSVJKOyrolB/TWzkKSXgiOiYjpVJUmoqmvWaSpJOwVHRJl6HJKEyromBYeknYIjojRXlSRDwSFhUHBElOaqkmRU1jUxqJ+CQ9JLwRFRGhyXZFTVNTFIPQ5JMwVHROk+DulOS9ypbtDguKSfgiOiYmaackS6VF2vm/8kHAqOiEpcjpvtKiSX6a5xCYuCI6I0OC7daQ0OjXFIuik4IsoMTasuXVKPQ8Ki4IiovJh6HNK1qrpmQMEh6afgiCidqpLuqMchYVFwRJSmHJHuKDgkLKEFh5mNN7PnzGyTmW0ws9s6aXeRma0O2jwfVj09jaYcke5U1jVRkGf0K9D7Q0mvMOdbbgbucPdVZjYQWGlmz7j7xtYGZjYYuAe41N13mdmIEOvpUXTnuHSndZ4qM8t2KdLDhPZWxN33u/uq4PNqYBMwtl2zvwaWuPuuoF1ZWPX0NJpWXbpTVa/pRiQcGenDmtkkYA6wot1D04EhZrbUzFaa2Y2ZqKcn0BiHdKdKM+NKSEJfGszMioBHgNvdvaqD1z8d+ADQH3jZzJa7++Z2+1gALACYMGFC2CVHgsY4pDuVdU0MGdAn22VIDxRqj8PMCkiExgPuvqSDJnuAJ929xt0PAy8Ap7Zv5O6L3H2uu88tKSkJs+TI0OW40h2txSFhCfOqKgPuBTa5+w87afZ74HwzyzezAcCZJMZCpBuJSQ6zXYXkMgWHhCXMU1XnAjcA68xsdbDt68AEAHdf6O6bzOxJYC0QB/7b3deHWFOPoWnVpSvurjEOCU1oweHuy4BurwN09x8APwirjp4qZkaLRselE8camom7bv6TcOjOoIiKxdTjkM69NTNu6Ne/SC+k4IgoDY5LVzTdiIRJwRFRuo9DuqK1OCRMCo6I0n0c0pXymkYAhhX2zXIl0hMpOCJKc1VJV1qDY2ihbgCU9FNwRJTmqpKuHDmWCI7BA3SqStJPwRFRGuOQrpTXNFLcv4CCPP2KS/rppyqiNMYhXSmvaWSYTlNJSBQcEaXLcaUrR2oaNL4hoVFwRJQGx6Ur5TWNCg4JTVLBYWa3mdkgS7jXzFaZ2fywi5POaa4q6Up5TSPDihQcEo5kexyfCdbSmA+UADcD/xpaVdKtmBnKDelIPO5U1DapxyGhSTY4WicrvBz4ubuvIYkJDCU8uhxXOlNZ10RL3Bmqm/8kJMkGx0oze5pEcDxlZgNJTIMuWaLBcelMeW3rXePqcUg4kp068xZgNvCmu9ea2VASp6skS0wLOUkndNe4hC3ZHsfZwBvuftTMPgX8H6AyvLKkO7qPQzrTete4gkPCkmxw/ASoNbNTgX8EdgL3h1aVdEuX40pnjk9wqKuqJCTJBkezJ97eXg3c5e53AQPDK0u6o4WcpDPlNQ2AehwSnmTHOKrN7J9IrCF+vpnlAZo9LYs0V5V05khNI0V98+mbn5ftUqSHSrbH8QmggcT9HAeAsWid8KzSGId0RneNS9iSCo4gLB4Ais3sSqDe3TXGkUW6HFc6o+CQsCU75ch1wCvAtcB1wAoz+3iYhUnXNDgunTlyTMEh4Up2jOMbwDx3LwMwsxLgT8BvwipMumaWmFpCpL3ymkZmjhmU7TKkB0t2jCPWGhqBIyk8V0KgU1XSEXfXWhwSumR7HE+a2VPAr4KvPwE8EU5JkozEXFXZrkJyzbGGZhpb4jpVJaFKKjjc/Stmdg1wLonJDRe5+29DrUy6pB6HdETTjUgmJNvjwN0fAR4JsRZJgWladenAEd01LhnQZXCYWTXQ0Z8nA9zdNQKXJXm6c1w6cLCyHoCSon5ZrkR6si6Dw901rUiO0qkq6cjWsmMATC4pzHIl0pPpyqiI0pQj0pHNZccYN6Q/hX2TPgstkjIFR0TFgvUXNe2ItLXlYDXTRhRluwzp4RQcERWzRHKo1yGtmlvivHmohukjdYZZwqXgiKjWHofGOaTVzvJaGlviTFNwSMgUHBFlx3scCg5J2HIwMTCuU1USNgVHRLWeqlJuSKstB6sBmKrgkJCFFhxmNt7MnjOzTWa2wcxu66LtPDNr0Yy7ydOpKmlvi66okgwJ8yesGbjD3VeZ2UBgpZk94+4b2zYKVhP8PvBUiLX0OBocl/Y264oqyZDQehzuvt/dVwWfVwObSKwc2N6XSExlUtbBY9IJU49D2mhuifPmYV1RJZmRkTEOM5sEzAFWtNs+FvgosLCb5y8ws1IzKz106FBodUbJ8TGOeJYLkZywq7yWxmZdUSWZEXpwmFkRiR7F7e5e1e7hHwFfdfeWrvbh7ovcfa67zy0pKQmp0mjRGIe0tWbPUQCmj9SpKglfqKNoZlZAIjQecPclHTSZCzwUXFo6HLjczJrd/Xdh1tUTxGK6HFfe8puVexg3pD+zxhRnuxTpBUILDkukwb3AJnf/YUdt3P2ENu3vAx5TaCTHNDgugd3ltby49Qh//8Hpx99QiIQpzB7HucANwDozWx1s+zowAcDduxzXkK5pripp9XDpbszg46ePy3Yp0kuEFhzuvozEuh3Jtr8prFp6Il2OKwAtcefhlXu4YFoJYwb3z3Y50kvozvGI0uC4APxlyyH2V9bziXnjs12K9CIKjojSXFUC8OiafQzql88HThqR7VKkF1FwRFReEBzNLQqO3qq+qYWnNxzk0lmj6Jufl+1ypBdRcETUqOLEmtJ7j9ZluRLJlqVvHOJYQzNXnTom26VIL6PgiKjWGVBb15iW3ucPa/cxrLAPZ08elu1SpJdRcETUiIF9Gdg3X8HRS9U0NPPspoNcfvJo8vP0ayyZpZ+4iDIzJo8oYtshBUdvtLh0N/VNca48ZXS2S5FeSMERYVNLitTj6IU27a/ie398nfOnDeeME4ZmuxzphRQcETZ1RBFl1Q1U1TdluxTJkJqGZr7w4CoG9y/gPz4x+/hl2SKZpOCIsNYB8m3qdfQKu47U8vGFL7PjcA0/un42w4v6Zrsk6aUUHBE2paQQ0JVVvcH6vZVc9eNl7K2o5d5Pz+OcKcOzXZL0YlqcOMImDB1An7wYWzVA3uPd99IO4u489qXzmDisMNvlSC+nHkeE5efFmDR8ANvKarJdioSouSXOs5sO8oEZIxQakhMUHBE3pSRxSW487uypqM12ORKClTsrqKhtYv77RmW7FBFAwRF5U0cUsfNIDfN/9ALnff85nnujLNslSZo9vfEgffJjXDBdyyZLblBwRNysscXEHfJjxvih/fn2HzbS2BzPdlmSJu7O0xsPcN7U4RT11ZCk5AYFR8TNnzmSZ758AX+87Xy+ffUsth+u4ecvbs92WZImrx+oZnd5HR+cOTLbpYgcp7cwEWdmTBs5EICLTxzB+2eM4O5nt3Cgqp7xQwawq7yWsup6PnPuCcydpLuMo+bh0j3EDK23ITlFwdHDfOvD7+OOxWt46JXd1DW10L8gj34FMZ7acJAvXzKNW86bTP8+WrshCnYdqeWXy3dw7enjGTGwX7bLETlOwdHDjB86gMW3nk087hypaWRYYR9qGpv5+m/X829Pb+buP2/lnCnDuPXCKZyl6bhz2veffJ38WIy/nz8926WIvI3GOHqoWMwoGdiXWMwY2K+Au6+fzYOfPZNPnjmB1/dXc/2i5XzpV6+xv1ILQeWiZVsO8/i6/fzNhZMZOUi9Dckt5hFbs3ru3LleWlqa7TIira6xhYXPb2Ph89uImfHF90/lurnjKRmouY+yrbE5zn/+eQv3LN3GuCH9eeLvzqdQV1NJGpjZSnefm5Z9KTh6r93ltfzL4xt5asNBAMYP7c/8maO45rRxTC4pJD9mWiQow/7xN2tYXLqHj58+jjuvmsmgfgXZLkl6CAWHgiOt1u2pZPmbR1ixvZznN5fR1PLWz8SMUQP54MyRfPjUMcev3pJwLNtymE/du4JbL5zC1y6bke1ypIdRcCg4QlNR08hTGw5QXttIfWMLy7eXU7qjnLjDaRMG8+lzJnHlKWPIi2kdiHSqbWzmQz96gYJYjCduO59+BbryTdIrncGhk6fyNkMK+3D9GRPetu3wsQZ+u2ovv3p1F7c9tJq7nt3CZ849gatOHUNxf51Kea82H6zmzt+vZ3d5Hb9ecJZCQ3KeehyStHjceXLDAf7zz1vZtL+KPvkxrjltHF/+4DTdZ/AuxOPOf/xpM/cs3UZhnzz+6fKT+Kt2oS2SLjpVpeDIKndn/d4qHnp1F79+dTd982N84KSRjBjYl9MmDmH+zJGRGVRvbolnpdZjDc18+dereWbjQa45bRzfuOIkhhb2yXgd0nsoOBQcOePNQ8f492c2s3bPUQ5VN1DfFGdMcT8+fc4krp83geIB2T2V1RL3d4zHvH6gin97ajOb9lex92gd00cWcfGJIzjjhKGcMm5wypclN7XEWb37KMu2HOblbUdobIkzpaSIWWMHcd7U4UwdUfS2tcEbmlu44b9fYeWuCv75ipP49DmTtHa4hE7BoeDISS1x57nXy/jZi9t5adsR+hfkcc3pY7nx7ElMz+AVWS1x5/nNZfzP8l0sfaOMySVFzJs0lNHF/aisa+IXL+1gUP8CLpg2nDGD+7Nmz1Fe2V5+/GqyU8cV85E5Y7nylDGdhkhzS5ylbxzi4ZW7WbblMDWNLZjByWOLKeqbz9ayY5RVNwCJqe+/ePFUrjxlNHkx447Fa1jy2l7uun42V88em7H/F+ndFBwKjpy3aX8VP1u2nd+v2Udjc5yzJg/lxrMn8YGTRtAnL/aOd9hV9U2s3nWUXeW1ODCoXz5nnjCMUcXJjZ3sr6zjzUM1vLargode3c2eijpKBvblipNHs/NIDat2HaWyrgmAj84Zy51XzmRIm1NDtY3NrN9bRenOch5fu58N+6rIixnnTxvOvElDmThsAHlmVNU38cr2Cp57o4zymkaGF/Xl0lkjOW9qCWdPHva2Htbu8lpe2HKIX768k9cPVDOgTx7Di/qyq7yWL18yndsumfbe/6NFkqTgUHBERnlNI4tLd/PLl3ey9+hb05sU9slj7JD+9C/I40BVPWXVDXT0ozilpJDzpg7nnKnDOWvysLddxVVWXc+vVuzm8XX72HzwrXXXz5o8lBvOmsT8942koM34RVNLnIbmeFLrWmw+WM3vXtvLY2v3s6v87SsrFvcv4P0zRnD5yaO56MSSt71GR+Jx50+bDrL8zXL2Hq1lxqhB3H7JNJ2ekoxScCg4Iqf1NNaGfVW0uFNV18Teo3XUN7UwalA/JgwdwJwJQ5g2soiYGQer6nl52xFe3HaYFW+WU9fUQswSC1edPLYYs8SU440tceZNHMqHZo3ipFEDmTKiKO1zO9U0NLO7ohZ3KOqbz+jifpEZ/BdppeBQcPQqjc3B4PPWw7yy/Qgb9lVR29jCx+aM5QsXT2XS8MJslyiS8yJxA6CZjQfuB0YBcWCRu9/Vrs0nga8GXx4DPu/ua8KqSaKpT36MM04YyhknJBaiisedhua41hURyZIw7xxvBu5w91VmNhBYaWbPuPvGNm22Axe6e4WZXQYsAs4MsSbpAWIxU2iIZFFoweHu+4H9wefVZrYJGAtsbNPmpTZPWQ6MC6seERFJj4yM8JnZJGAOsKKLZrcAf8xEPSIi8u6FPsmhmRUBjwC3u3tVJ20uJhEc53Xy+AJgAcCECZrLR0Qkm0LtcZhZAYnQeMDdl3TS5hTgv4Gr3f1IR23cfZG7z3X3uSUlJeEVLCIi3QotOCxxd9O9wCZ3/2EnbSYAS4Ab3H1zWLWIiEj6hHmq6lzgBmCdma0Otn0dmADg7guBO4FhwD3BXbTN6brOWEREwhHmVVXLgC7nVHD3zwKfDasGERFJP82bICIiKVFwiIhIShQcIiKSEgWHiIikRMEhIiIpUXCIiEhKFBwiIpISBYeIiKREwSEiIilRcIiISEoUHCIikhIFh4iIpETBISIiKVFwiIhIShQcIiKSEgWHiIikRMEhIiIpUXCIiEhKFBwiIpISBYeIiKREwSEiIilRcIiISEoUHCIikhIFh4iIpETBISIiKVFwiIhIShQcIiKSEgWHiIikRMEhIiIpUXCIiEhKFBwiIpISBYeIiKREwSEiIilRcIiISEoUHCIikpLQgsPMxpvZc2a2ycw2mNltHbQxM7vbzLaa2VozOy2sekREJD3yQ9x3M3CHu68ys4HASjN7xt03tmlzGTAt+DgT+Enwr4iI5KjQehzuvt/dVwWfVwObgLHtml0N3O8Jy4HBZjY6rJpEROS9C7PHcZyZTQLmACvaPTQW2N3m6z3Btv3tnr8AWBB82WBm68OpNCXFQGWW95XK85Jp212bzh5PZftw4HA3dYQtncfuvewvk8cv1cdy9dhB9I5frvzundhNDclz91A/gCJgJfCxDh57HDivzdfPAqd3s7/SsGtO8vtalO19pfK8ZNp216azx1PZngvHL53HLirHL9XHcvXYRfH49cTfvVCvqjKzAuAR4AF3X9JBkz3A+DZfjwP2hVlTGv0hB/aVyvOSadtdm84eT3V7tqW7rigcv1Qfy9VjB9E7fj3ud8+CJEr/js0M+AVQ7u63d9LmCuCLwOUkBsXvdvczutlvqbvPTXO5kiE6ftGlYxdt6Tx+YY5xnAvcAKwzs9XBtq8DEwDcfSHwBInQ2ArUAjcnsd9Faa9UMknHL7p07KItbccvtB6HiIj0TLpzXEREUqLgEBGRlCg4REQkJT0qOMzsIjP7i5ktNLOLsl2PpMbMCs1spZldme1aJDVmdlLwe/cbM/t8tuuR1JjZR8zsp2b2ezOb3137nAkOM/uZmZW1vyvczC41szeCiRC/1s1uHDgG9CNxj4hkQJqOHcBXgcXhVCmdScfxc/dN7n4rcB2gS3YzKE3H73fu/jngJuAT3b5mrlxVZWYXkPijf7+7zwq25QGbgQ+SCIJXgb8C8oDvtdvFZ4DD7h43s5HAD939k5mqvzdL07E7hcSUFv1IHMfHMlO9pOP4uXuZmX0Y+BrwY3d/MFP193bpOn7B8/6dxA3bq7p6zYzMVZUMd38hmNOqrTOAre7+JoCZPQRc7e7fA7o6nVEB9A2lUHmHdBw7M7sYKARmAnVm9oS7x8OtXCB9v3vu/ijwqJk9Dig4MiRNv38G/Cvwx+5CA3IoODrR0SSInU67bmYfAz4EDAZ+HGpl0p2Ujp27fwPAzG4i6DmGWp10J9XfvYuAj5F4w/ZEmIVJUlI6fsCXgEuAYjObGtyg3alcDw7rYFun59aC+bA6mhNLMi+lY3e8gft96S9F3oVUf/eWAkvDKkZSlurxuxu4O9md58zgeCeiPAlib6djF206ftEW6vHL9eB4FZhmZieYWR/geuDRLNckydGxizYdv2gL9fjlTHCY2a+Al4ETzWyPmd3i7s0kZs99isQKgovdfUM265R30rGLNh2/aMvG8cuZy3FFRCQacqbHISIi0aDgEBGRlCg4REQkJQoOERFJiYJDRERSouAQEZGUKDgkdGZ2LAOvcauZ3Rj267R7zY+Y2cx3+bw7g8//r5n9Q/qrS12wnk2XsxKb2clmdl+GSpIcletzVYkcZ2Z57t7S0WPdTcoWxmsCHwEeAzamuNt/BD78XurKFndfZ2bjzGyCu+/Kdj2SHepxSEaZ2VfM7FUzW2tm32qz/XfB6n8bzGxBm+3HzOzbZrYCODv4+v+Z2RozWx6svfK2d+5mttTMvm9mr5jZZjM7P9g+wMwWB6/9azNbYWbvWHTIzHaY2Z1mtgy41sw+F9S8xsweCfZzDok//j8ws9VmNiX4eDL4Pv5iZjM62Pd0oMHdD3fw2Ozge1prZr81syHB9nnBtpfN7AfWbsGeoM1oM3shqGV9m+/5UjNbFdT+bLDtDDN7ycxeC/49sYP9FVpigaBXg3ZXt3n4DySmsJBeSsEhGWOJJSmnkVgrYDZwuiUWoYHEYjKnk1g97u/MbFiwvRBY7+5nuvuy4Ovl7n4q8ALwuU5eLt/dzwBuB74ZbPtboMLdTwG+A5zeRbn17n6euz8ELHH3ecFrbgJucfeXSMz98xV3n+3u24BFwJeC7+MfgHs62O+5QGfrHdwPfDWob12bun8O3OruZwOd9X7+GnjK3WcDpwKrzawE+ClwTVD7tUHb14EL3H0OcCfw3Q729w3gz+4+D7iYREAWBo+VAud3Uof0AjpVJZk0P/h4Lfi6iESQvEAiLD4abB8fbD9C4g/lI2320Uji9BDAShIrnHVkSZs2k4LPzwPuAnD39Wa2totaf93m81lm9i8k1nkpIjH/z9uYWRFwDvCw2fEZrTtaTGw0cKiD5xcDg939+WDTL4J9DQYGBkEFiQWSOlpI6VXgZ2ZWAPzO3VdbYo2MF9x9O4C7lwdti4FfmNk0ElNtF3Swv/nAh9uMv/QDJpAIzjJgTAfPkV5CwSGZZMD33P2/3rYx8QfuEuBsd681s6Uk/lBB4p1/23fZTf7WBGstdP4z3NBBm47WKOhMTZvP7wM+4u5rLLHQ1EUdtI8BR4N3/F2pI/GHO1lJ1RysAncBcAXwSzP7AXCUjtdg+A7wnLt/1BIrxy3t5HWvcfc3OnisH4nvQ3opnaqSTHoK+Ezw7hwzG2tmI0j8Ia0IQmMGcFZIr78MuC547ZnAyUk+byCwP3g333Yd++rgMdy9CthuZtcG+zczO7WDfW0Cprbf6O6VQEXr2ARwA/C8u1cA1WbW+n/S4diCmU0Eytz9p8C9wGkkZky90MxOCNoMDZoXA3uDz2/q5Ht+CviSBd0nM5vT5rHpwDvGWaT3UHBIxrj70yROtbxsZuuA35D4w/skkB+cOvoOsDykEu4BSoLX+SqwFqhM4nn/DKwAniExPtDqIeArweDxFBKhcouZrQE2AFe/Y0+J03JzWv8gt/NpEmMJa0mMAX072H4LsMjMXibRE+io5otIjGu8BlwD3OXuh4AFwJKgptbTb/8f+J6ZvQjkdfI9f4fEKay1wWD8d9o8djHweCfPk15A06pLr2FmeUCBu9cHf+ifBaa7e2OG67gL+IO7/ynJ9kXufiz4/GvAaHe/Lcwau6ilL/A8cF6w5oP0QhrjkN5kAPBccMrJgM9nOjQC3wXOTKH9FWb2TyR+X3fS+emlTJgAfE2h0bupxyEiIinRGIeIiKREwSEiIilRcIiISEoUHCIikhIFh4iIpETBISIiKflfU1Fu8cdL23cAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder.plot_loss(xlim=(1e-5, 1e-2), ylim=(2, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.9996 - accuracy: 0.2728 - val_loss: 1.8797 - val_accuracy: 0.3226\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.7850 - accuracy: 0.3539 - val_loss: 1.7658 - val_accuracy: 0.3530\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6964 - accuracy: 0.3918 - val_loss: 1.7924 - val_accuracy: 0.3576\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6386 - accuracy: 0.4138 - val_loss: 1.6854 - val_accuracy: 0.3888\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5942 - accuracy: 0.4288 - val_loss: 1.6369 - val_accuracy: 0.4094\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5568 - accuracy: 0.4429 - val_loss: 1.5939 - val_accuracy: 0.4240\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5246 - accuracy: 0.4530 - val_loss: 1.5934 - val_accuracy: 0.4254\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4980 - accuracy: 0.4632 - val_loss: 1.6399 - val_accuracy: 0.4126\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4705 - accuracy: 0.4756 - val_loss: 1.5873 - val_accuracy: 0.4310\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4482 - accuracy: 0.4837 - val_loss: 1.6149 - val_accuracy: 0.4274\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4309 - accuracy: 0.4909 - val_loss: 1.5918 - val_accuracy: 0.4358\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4123 - accuracy: 0.4948 - val_loss: 1.5550 - val_accuracy: 0.4490\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3899 - accuracy: 0.5055 - val_loss: 1.5760 - val_accuracy: 0.4326\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3790 - accuracy: 0.5061 - val_loss: 1.5208 - val_accuracy: 0.4580\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3658 - accuracy: 0.5101 - val_loss: 1.5376 - val_accuracy: 0.4516\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3511 - accuracy: 0.5162 - val_loss: 1.5267 - val_accuracy: 0.4610\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3365 - accuracy: 0.5224 - val_loss: 1.5330 - val_accuracy: 0.4456\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3211 - accuracy: 0.5284 - val_loss: 1.5218 - val_accuracy: 0.4646\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3039 - accuracy: 0.5344 - val_loss: 1.6176 - val_accuracy: 0.4354\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2940 - accuracy: 0.5358 - val_loss: 1.5268 - val_accuracy: 0.4618\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2785 - accuracy: 0.5418 - val_loss: 1.5087 - val_accuracy: 0.4728\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2591 - accuracy: 0.5481 - val_loss: 1.5171 - val_accuracy: 0.4754\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2482 - accuracy: 0.5534 - val_loss: 1.5068 - val_accuracy: 0.4698\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.2323 - accuracy: 0.5593 - val_loss: 1.5015 - val_accuracy: 0.4760\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2190 - accuracy: 0.5643 - val_loss: 1.5043 - val_accuracy: 0.4700\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2052 - accuracy: 0.5690 - val_loss: 1.4951 - val_accuracy: 0.4862\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1890 - accuracy: 0.5764 - val_loss: 1.5537 - val_accuracy: 0.4526\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1731 - accuracy: 0.5794 - val_loss: 1.4923 - val_accuracy: 0.4700\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1587 - accuracy: 0.5842 - val_loss: 1.4709 - val_accuracy: 0.4846\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1421 - accuracy: 0.5918 - val_loss: 1.4837 - val_accuracy: 0.4824\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1294 - accuracy: 0.5960 - val_loss: 1.4938 - val_accuracy: 0.4880\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1094 - accuracy: 0.6022 - val_loss: 1.5088 - val_accuracy: 0.4734\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0969 - accuracy: 0.6079 - val_loss: 1.4810 - val_accuracy: 0.4904\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0786 - accuracy: 0.6136 - val_loss: 1.5111 - val_accuracy: 0.4890\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0651 - accuracy: 0.6200 - val_loss: 1.5191 - val_accuracy: 0.4860\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0498 - accuracy: 0.6203 - val_loss: 1.5461 - val_accuracy: 0.4796\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0339 - accuracy: 0.6293 - val_loss: 1.5662 - val_accuracy: 0.4796\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0213 - accuracy: 0.6341 - val_loss: 1.5510 - val_accuracy: 0.4830\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0052 - accuracy: 0.6416 - val_loss: 1.5726 - val_accuracy: 0.4778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3b438b9e8>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(base_cifar.optimizer.learning_rate, 1e-4)\n",
    "base_cifar.fit(\n",
    "    X_train, y_train, epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "88/88 [==============================] - 9s 42ms/step - loss: 3.2480 - accuracy: 0.0952\n",
      "Epoch 2/5\n",
      "88/88 [==============================] - 4s 40ms/step - loss: 2.3327 - accuracy: 0.2192\n",
      "Epoch 3/5\n",
      "88/88 [==============================] - 4s 40ms/step - loss: 2.0069 - accuracy: 0.2995\n",
      "Epoch 4/5\n",
      "88/88 [==============================] - 4s 40ms/step - loss: 117.7062 - accuracy: 0.1161\n",
      "Epoch 5/5\n",
      "88/88 [==============================] - 4s 40ms/step - loss: 44831.0938 - accuracy: 0.0997\n"
     ]
    }
   ],
   "source": [
    "bn_cifar = build_cifar(batch_normalization=True)\n",
    "lr_finder = LRFinder(bn_cifar)\n",
    "lr_finder.find(X_train, y_train, start_lr=1e-6, end_lr=100, batch_size=512, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder.plot_loss(xlim=(1e-6, 1), ylim=(0,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.9971 - accuracy: 0.2861 - val_loss: 1.8837 - val_accuracy: 0.3278\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.8719 - accuracy: 0.3291 - val_loss: 1.7761 - val_accuracy: 0.3558\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.7855 - accuracy: 0.3598 - val_loss: 1.7333 - val_accuracy: 0.3712\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.7286 - accuracy: 0.3819 - val_loss: 1.6729 - val_accuracy: 0.3914\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.6874 - accuracy: 0.3987 - val_loss: 1.6277 - val_accuracy: 0.4142\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.6604 - accuracy: 0.4084 - val_loss: 1.5907 - val_accuracy: 0.4332\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.6319 - accuracy: 0.4197 - val_loss: 1.6221 - val_accuracy: 0.4302\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.6034 - accuracy: 0.4330 - val_loss: 1.6038 - val_accuracy: 0.4222\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.5816 - accuracy: 0.4407 - val_loss: 1.5618 - val_accuracy: 0.4452\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.5591 - accuracy: 0.4519 - val_loss: 1.5593 - val_accuracy: 0.4384\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.5321 - accuracy: 0.4603 - val_loss: 1.5525 - val_accuracy: 0.4532\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.5073 - accuracy: 0.4680 - val_loss: 1.5181 - val_accuracy: 0.4760\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.4835 - accuracy: 0.4774 - val_loss: 1.4956 - val_accuracy: 0.4726\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.4601 - accuracy: 0.4852 - val_loss: 1.4851 - val_accuracy: 0.4684\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.4332 - accuracy: 0.4950 - val_loss: 1.4625 - val_accuracy: 0.4882\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.4059 - accuracy: 0.5083 - val_loss: 1.4526 - val_accuracy: 0.4876\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.3792 - accuracy: 0.5154 - val_loss: 1.4527 - val_accuracy: 0.4912\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.3667 - accuracy: 0.5215 - val_loss: 1.4293 - val_accuracy: 0.4990\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.3375 - accuracy: 0.5288 - val_loss: 1.4455 - val_accuracy: 0.4900\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.3216 - accuracy: 0.5384 - val_loss: 1.3916 - val_accuracy: 0.5156\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.2995 - accuracy: 0.5448 - val_loss: 1.3997 - val_accuracy: 0.5112\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2817 - accuracy: 0.5527 - val_loss: 1.3845 - val_accuracy: 0.5124\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2599 - accuracy: 0.5593 - val_loss: 1.3783 - val_accuracy: 0.5174\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2469 - accuracy: 0.5659 - val_loss: 1.3992 - val_accuracy: 0.5188\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2312 - accuracy: 0.5708 - val_loss: 1.4007 - val_accuracy: 0.5098\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2142 - accuracy: 0.5748 - val_loss: 1.4048 - val_accuracy: 0.5190\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.2009 - accuracy: 0.5800 - val_loss: 1.4150 - val_accuracy: 0.5108\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1824 - accuracy: 0.5910 - val_loss: 1.3877 - val_accuracy: 0.5250\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1712 - accuracy: 0.5924 - val_loss: 1.3750 - val_accuracy: 0.5190\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.1648 - accuracy: 0.5965 - val_loss: 1.3940 - val_accuracy: 0.5134\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1437 - accuracy: 0.6032 - val_loss: 1.3722 - val_accuracy: 0.5310\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1359 - accuracy: 0.6053 - val_loss: 1.3628 - val_accuracy: 0.5272\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.1197 - accuracy: 0.6108 - val_loss: 1.3741 - val_accuracy: 0.5228\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.1102 - accuracy: 0.6149 - val_loss: 1.4018 - val_accuracy: 0.5188\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1002 - accuracy: 0.6186 - val_loss: 1.3890 - val_accuracy: 0.5326\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.0892 - accuracy: 0.6214 - val_loss: 1.4072 - val_accuracy: 0.5160\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.0836 - accuracy: 0.6217 - val_loss: 1.3882 - val_accuracy: 0.5246\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.0617 - accuracy: 0.6317 - val_loss: 1.4229 - val_accuracy: 0.5178\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.0610 - accuracy: 0.6331 - val_loss: 1.4121 - val_accuracy: 0.5232\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.0469 - accuracy: 0.6358 - val_loss: 1.3905 - val_accuracy: 0.5198\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.0396 - accuracy: 0.6383 - val_loss: 1.3958 - val_accuracy: 0.5288\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.0390 - accuracy: 0.6393 - val_loss: 1.4076 - val_accuracy: 0.5214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff468b9cc50>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(bn_cifar.optimizer.learning_rate, 1e-3)\n",
    "bn_cifar.fit(\n",
    "    X_train, y_train, epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Try replacing Batch Normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = (X_train - X_train.mean()) / X_train.std()\n",
    "X_valid_scaled = (X_valid - X_valid.mean()) / X_valid.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "88/88 [==============================] - 4s 25ms/step - loss: 2.5615 - accuracy: 0.1274\n",
      "Epoch 2/5\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 2.5247 - accuracy: 0.1758\n",
      "Epoch 3/5\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 7295250432.0000 - accuracy: 0.0975\n"
     ]
    }
   ],
   "source": [
    "norm_cifar = build_cifar(activation='selu', kernel_initializer='lecun_normal')\n",
    "lr_finder = LRFinder(norm_cifar)\n",
    "lr_finder.find(X_train, y_train, start_lr=1e-6, end_lr=100, batch_size=512, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder.plot_loss()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.8899 - accuracy: 0.3216 - val_loss: 1.9213 - val_accuracy: 0.2854\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.7152 - accuracy: 0.3836 - val_loss: 1.7453 - val_accuracy: 0.3826\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6396 - accuracy: 0.4189 - val_loss: 1.7026 - val_accuracy: 0.3982\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5942 - accuracy: 0.4340 - val_loss: 1.6451 - val_accuracy: 0.4204\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5572 - accuracy: 0.4479 - val_loss: 1.6641 - val_accuracy: 0.4072\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5217 - accuracy: 0.4629 - val_loss: 1.7025 - val_accuracy: 0.4028\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5019 - accuracy: 0.4706 - val_loss: 1.6140 - val_accuracy: 0.4342\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4771 - accuracy: 0.4784 - val_loss: 1.6337 - val_accuracy: 0.4126\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4554 - accuracy: 0.4870 - val_loss: 1.6181 - val_accuracy: 0.4336\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4394 - accuracy: 0.4896 - val_loss: 1.6226 - val_accuracy: 0.4262\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4231 - accuracy: 0.4993 - val_loss: 1.6083 - val_accuracy: 0.4342\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4083 - accuracy: 0.5044 - val_loss: 1.6184 - val_accuracy: 0.4338\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4014 - accuracy: 0.5070 - val_loss: 1.6235 - val_accuracy: 0.4432\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3947 - accuracy: 0.5091 - val_loss: 1.5865 - val_accuracy: 0.4476\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3869 - accuracy: 0.5135 - val_loss: 1.6045 - val_accuracy: 0.4400\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3738 - accuracy: 0.5178 - val_loss: 1.6045 - val_accuracy: 0.4430\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3609 - accuracy: 0.5239 - val_loss: 1.5624 - val_accuracy: 0.4596\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3497 - accuracy: 0.5284 - val_loss: 1.5763 - val_accuracy: 0.4490\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3335 - accuracy: 0.5381 - val_loss: 1.5831 - val_accuracy: 0.4550\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3236 - accuracy: 0.5392 - val_loss: 1.5861 - val_accuracy: 0.4580\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3108 - accuracy: 0.5473 - val_loss: 1.6086 - val_accuracy: 0.4596\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3021 - accuracy: 0.5501 - val_loss: 1.5771 - val_accuracy: 0.4618\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3059 - accuracy: 0.5483 - val_loss: 1.6199 - val_accuracy: 0.4614\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2932 - accuracy: 0.5552 - val_loss: 1.6251 - val_accuracy: 0.4636\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2962 - accuracy: 0.5532 - val_loss: 1.5747 - val_accuracy: 0.4730\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5510 - accuracy: 0.4862 - val_loss: 1.9002 - val_accuracy: 0.3002\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5751 - accuracy: 0.4346 - val_loss: 1.6056 - val_accuracy: 0.4248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3013f0dfd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(norm_cifar.optimizer.learning_rate, 1e-3)\n",
    "norm_cifar.fit(\n",
    "    X_train_scaled, y_train, epochs=100,\n",
    "    validation_data=(X_valid_scaled, y_valid),\n",
    "    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "88/88 [==============================] - 6s 40ms/step - loss: 3.7121 - accuracy: 0.0992\n",
      "Epoch 2/5\n",
      "88/88 [==============================] - 3s 40ms/step - loss: 3.2130 - accuracy: 0.0986\n",
      "Epoch 3/5\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 8829256335360.0000 - accuracy: 0.1025\n"
     ]
    }
   ],
   "source": [
    "drop_cifar = build_cifar(dropout=True)\n",
    "lr_finder = LRFinder(drop_cifar)\n",
    "lr_finder.find(X_train, y_train, start_lr=1e-6, end_lr=100, batch_size=512, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder.plot_loss(xlim=(2e-2,6e-2), ylim=(2,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 210198432.0000 - accuracy: 0.1002 - val_loss: 2.3095 - val_accuracy: 0.0952\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 692484.1875 - accuracy: 0.0999 - val_loss: 2.3173 - val_accuracy: 0.1018\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 77.6134 - accuracy: 0.0996 - val_loss: 2.3144 - val_accuracy: 0.0972\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 4.2360 - accuracy: 0.1023 - val_loss: 2.3131 - val_accuracy: 0.1008\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 3.6521 - accuracy: 0.1016 - val_loss: 2.3229 - val_accuracy: 0.1054\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 66.4436 - accuracy: 0.1005 - val_loss: 2.3123 - val_accuracy: 0.1018\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 6.6484 - accuracy: 0.1016 - val_loss: 2.3235 - val_accuracy: 0.0972\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 4.5436 - accuracy: 0.1006 - val_loss: 2.3308 - val_accuracy: 0.1054\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 4.2111 - accuracy: 0.0987 - val_loss: 2.3211 - val_accuracy: 0.1018\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 9.9824 - accuracy: 0.1005 - val_loss: 2.3419 - val_accuracy: 0.1054\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 85362679808.0000 - accuracy: 0.1007 - val_loss: 2.3565 - val_accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f30101a72e8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(drop_cifar.optimizer.learning_rate, 5e-2)\n",
    "drop_cifar.fit(\n",
    "    X_train, y_train, epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([drop_cifar(X_valid, training=False) for _ in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0952\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "for i, instance in enumerate(y_proba):\n",
    "    if y_valid[i] == instance.argmax():\n",
    "        score += 1\n",
    "print(score / len(X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_cycle_schedule(lrmin=1e-6, lrmax=1e-2, time=20):\n",
    "    def one_cycle_fn(epoch):\n",
    "        climb = (lrmax-lrmin) / time\n",
    "        if epoch < time:\n",
    "            return lrmin + climb * epoch\n",
    "        else:\n",
    "            lr = lrmax - climb * (epoch-time)\n",
    "            return lr if lr < lrmin else lr / 10.0\n",
    "    return one_cycle_fn\n",
    "\n",
    "one_cycle_cb = keras.callbacks.LearningRateScheduler(one_cycle_schedule(lrmin=1e-5, lrmax=1e-4, time=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 14s 9ms/step - loss: 2.0905 - accuracy: 0.2488 - val_loss: 1.9122 - val_accuracy: 0.3040\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.8392 - accuracy: 0.3319 - val_loss: 1.8219 - val_accuracy: 0.3378\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.7467 - accuracy: 0.3684 - val_loss: 1.7547 - val_accuracy: 0.3558\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6809 - accuracy: 0.3919 - val_loss: 1.6879 - val_accuracy: 0.3916\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.6292 - accuracy: 0.4106 - val_loss: 1.6345 - val_accuracy: 0.4180\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5873 - accuracy: 0.4261 - val_loss: 1.6649 - val_accuracy: 0.4028\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.5520 - accuracy: 0.4414 - val_loss: 1.6160 - val_accuracy: 0.4128\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.5214 - accuracy: 0.4543 - val_loss: 1.5631 - val_accuracy: 0.4402\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4979 - accuracy: 0.4632 - val_loss: 1.6107 - val_accuracy: 0.4224\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4767 - accuracy: 0.4701 - val_loss: 1.5787 - val_accuracy: 0.4384\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4569 - accuracy: 0.4780 - val_loss: 1.5248 - val_accuracy: 0.4574\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4386 - accuracy: 0.4828 - val_loss: 1.5457 - val_accuracy: 0.4450\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4220 - accuracy: 0.4907 - val_loss: 1.5300 - val_accuracy: 0.4612\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4032 - accuracy: 0.4972 - val_loss: 1.5344 - val_accuracy: 0.4502\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3885 - accuracy: 0.5022 - val_loss: 1.5977 - val_accuracy: 0.4282\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3726 - accuracy: 0.5086 - val_loss: 1.4806 - val_accuracy: 0.4650\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3565 - accuracy: 0.5151 - val_loss: 1.4514 - val_accuracy: 0.4742\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3427 - accuracy: 0.5202 - val_loss: 1.4932 - val_accuracy: 0.4704\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3240 - accuracy: 0.5247 - val_loss: 1.4849 - val_accuracy: 0.4712\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3127 - accuracy: 0.5304 - val_loss: 1.4892 - val_accuracy: 0.4636\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1690 - accuracy: 0.5821 - val_loss: 1.4225 - val_accuracy: 0.4946\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1478 - accuracy: 0.5907 - val_loss: 1.4131 - val_accuracy: 0.5012\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1368 - accuracy: 0.5958 - val_loss: 1.4259 - val_accuracy: 0.5014\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1286 - accuracy: 0.5986 - val_loss: 1.4138 - val_accuracy: 0.5018\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1226 - accuracy: 0.6002 - val_loss: 1.4189 - val_accuracy: 0.5022\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1152 - accuracy: 0.6022 - val_loss: 1.4224 - val_accuracy: 0.5042\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1084 - accuracy: 0.6056 - val_loss: 1.4263 - val_accuracy: 0.5014\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1035 - accuracy: 0.6076 - val_loss: 1.4209 - val_accuracy: 0.5056\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.0986 - accuracy: 0.6084 - val_loss: 1.4221 - val_accuracy: 0.5064\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.0944 - accuracy: 0.6106 - val_loss: 1.4296 - val_accuracy: 0.5048\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.0897 - accuracy: 0.6124 - val_loss: 1.4268 - val_accuracy: 0.5042\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.0847 - accuracy: 0.6154 - val_loss: 1.4328 - val_accuracy: 0.5008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2fe63cf0f0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_cifar = build_cifar()\n",
    "oc_cifar.fit(\n",
    "    X_train, y_train, epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping_cb, one_cycle_cb]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "741656c466d97e6cc8d57289f00cb22b14e49f976d4b6ab783fc4f7e7dcfa85a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
