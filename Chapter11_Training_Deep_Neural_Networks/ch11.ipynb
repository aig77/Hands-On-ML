{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training Deep Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation works by going from the output layer to the input layer, propagating the error gradient along the way. After computing the gradient of the cost function with regard to each parameter in the network, it uses the gradients to update each parameter with a Gradient Descent step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients often get smaller as the algorithm progresses down to lower layers. As a result, the Gradient Descent update leabes the lower layers' connection weights virtually unchanged, and training never converges to a good solution. This is the *vanishing gradients* problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the opposite can happen: the gradients can grow bigger until layers get insanely large weight updates and the algorithm diverges. This is the *exploding gradients* problem (recurrent neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at logistic activation function, when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is nothing left for the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorot and Bengio proposed that the signal needs to flow properly in both directions: in the foward direction when making predictions, and the reverse direction when backpropagating gradients. For the signal to flow properly, they argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing througha layer in the reverse direction. It's not possible to gurantee both, so they proposed a compromise. Connection weights of each layer must be initialized randomly using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$fan_{avg} = (fan_{in} + fan_{out}) / 2$\n",
    "\n",
    "|Initialization|Activation Functions|$\\sigma^{2}$ (Normal)|\n",
    "|--------------|:-------------------|:-------------------|\n",
    "|Glorot |None, tanh, logistic, softmax| $1/fan_{avg}$|\n",
    "|He|ReLU and variants| $2/fan_{in}$|\n",
    "|LeCunn|SELU|$1/fan_{in}$|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing $fan_{avg}$ with $fan_{in}$ yields LeCunn initialization. LeCunn and Glorot initialization are equivalent when $fan_{in}$ = $fan_{out}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Keras uses Glorot with a uniform distribution. Can be changed to He initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For He initialization with uniform dist but based on $fan_{avg}$ rather than $fan_{in}$, use `VarianceScaling`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_avg_init = keras.initializer.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other activation functions (besides sigmoid) work better on Deep Networks. Especially ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU isn't perfect though. It suffers from *dying ReLUs*: during training, some neurons \"die\" meaning they stop outputting anything other than 0. A neuron dies when its weights are tweaked in a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it keeps outputting zeros, and Gradient Descent does not affect it anymore beacuse the gradient of ReLU is zero when its input is negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LeakyReLU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this, use *Leaky ReLU*. Ensures that neurons never die. Defined as:<br>\n",
    "$LeakyReLU_{\\alpha}(z) = max(\\alpha z, z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ defines how much the function \"leaks\": it's the slope of the function for z<0 and is typically set to 0.01. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PReLU also outperforms ReLU in many cases. $\\alpha$ is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outperforms all ReLU variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![elu_formula](elu_form.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![elu](ELU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main drawback is that it is slower to compute than ReLU (due to the use of exponential function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SELU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled variant of ELU. Authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will *self-normalize*: the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, it outperforms other activations. However, some conditions must be met for self-normalization to happen:\n",
    "- Input features must be standardized ($\\mu$=0, $\\sigma$=1)\n",
    "- Every hidden layer's weights must be initialized with LeCunn normal initialization. In Keras, `kernel_initializer=\"lecun_normal\"`\n",
    "- Network architecture must be sequential\n",
    "- All layers are dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use leaky ReLU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    [...]\n",
    "    keras.layer.Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PReLU, replace `LeakyReLU(alpha=0.2)` with `PReLU()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SELU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly reduces possibility of vanishing/exploding gradients. Consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. I.e., the operation lets the model learn the optimal scale and mean of each of the layer's inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Led to huge improvement in the ImageNet classification task (large database of images classified into many classes, commonly used to evaluate computer vision systems). Vanishing gradients problem was strongly reduced, to the point they could use saturating activation functions such as tanh and logistic activation function. Networks were also much less sensitive to weight initialization. They were able to use larger learning rates, significantly speeding up the learning process. It also acts like a regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Batch Normalization with Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `BatchNormalization` layer before or after each hidden layer's activation function; optionally add BN layer as the first layer in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> [(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add BN layers before the activation functions, remove activation function from the hidden layers and add them as separate layers after the BN layers. Moreover, since BN layers include one offset parameter per input, you can remove the bias term from previous layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(10, 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update `momentum` hyperparameter for BN. It is used by BN when it updates the exponential moving average. Good values are close to 1; e.g, 0.9, 0.99, 0.999 (you want more 9s for larger datasets and smaller mini-batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`axis` hyperparameter determines which axis should be normalized. Defaults to -1, meaning that it will normalize the last axis (using the means and std computed across the *other* axes). When inpt batch is 2D ([*batch size, features*]), each input feature will be normalized based on the mean and std computed across all the instances in the batch. E.g., first BN layer in the previous code example will independetly normalize (and rescale and shift) 784 input features. If we move the first BN layer before the `Flatten` layer, the input batches will be 3D ([*batch size, height, width*]); therefore, the BN layer will compute 28 means and 28 std (1 per column of pixels, compputed across all instances in the batch and across all rows in the column). If you want to treat each of the 784 pixes independently, set `axis=[1, 2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip gradients during backpropagation so they never exceed some threshold; another technique to mitigate the exploding gradients. Often used in RNN, since BN is tricky to use in RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer will clip every component of the gradient vector to a value between -1.0 and 1.0. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, clip by norm by setting `clipnorm` instead of `clipvalue`. This will clip the whole gradient if its $l_{2}$ norm is greater than the threshold you picked. If gradients explode during training, try both clipping by value and nrom, with different thresholds, see which option performs best on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Transfer Learning*: resuing lower layers of a prexisting (similar) network to accomplish a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more similar the tasks are the more layers you want to reuse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the Fashion MNIST dataset contained only 8 classes (all the classes except shirt and sandal). Someone built and trained a Keras model on that set and got good performance (>90% accuracy). Call this model A. You want to tackle a different task: you have images of sandals and shirts, and you want to train a binary classifier (0=shirt, 1=sandal). Your dataset is quite small (200 labeled images). When you train a new model for this task (model B) with the same architecture as model A, it performs well (97% accuracy). Since it's a similar task to model A, transfer learning might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model('my_model_A.h5')\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) # reuse all the layers except output layer\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note new model shares layers with model A, so training will affect the layers in model A as well. You must clone model A to use its layers. Clone model A's architecture with `clone_model()` then copy its weights (`clone_model()` does not clone weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train `model_B_on_A` for task B. But output layer was initialized randomly, it will make large errors, so there will be large error gradients that may wreck the reused weights. To avoid, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. Set every layer's `trainable` attribute to `False` and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train the model for the first few epochs, then unfreeze the reused layers (requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing layers, reduce the learning rate, once again to avoid damaging the reused weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=[X_valid_B, y_valid_B])\n",
    "\n",
    "for layer in model_B_on_A[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimzer = keras.optimizers.SGD(lr=1e-4) # default lr is 1e-2\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning works best with deep CNN, whcih tend to learn feature detectors that are much more general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can gather unlabeled training data, you can use it to train an unsupervised model, such as an autoencoder or generative adversarial network. Useful when you have a complex task to solve, no similar model you can reuse, and little labeled training data but plenty of unlabeled training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining on an Auxiliary Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have much labeled training data, one option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for you actual task. The first neural network's lower layers will learn feature detectors that will likely be reusable by the second neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Momentum optimization\n",
    "- Nesterov Accelerated Gradient\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam and Nadam optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where Gradient Descent is like taking small steps to find out which direction to go toward, momentum optimization is like rolling a bowling ball down a gentle slope. It'll pick up momentum until it finds a terminal velocity (if there is some friction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cares about what the previous gradients were: at each iteration it subtracts the local gradient from the *momentum vector* **m** and it updates the weights by adding this momentum vector. Gradient is used for acceleration not speed. Algorithm introduces hyperparameter $\\beta$ for friction set between 0 (high friction) and 1 (no friction). Typical value is 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizer.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawback is that it adds another hyperparameter to tune. But 0.9 is usually fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant of momentum optimization. Measures a gradient of the cost function not at the local position, but slightly ahead in the direction of the momentum. Generally faster than momentum optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizer.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scales down the gradient vector along the steepest dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm decays the learning rate, but does so faster for steep dimensions than for dimensions with gentler slopes. This is called an *adaptive learning rate*. It helps the resulting updates more directly toward the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs well for simple quadratic problems, but often stops too early when training neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixes AdaGrad from stopping early by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). Decay rate $\\beta$ is usually 0.9. Again, another hyperparameter to tune but the default value usually works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note `rho` corresponds to $\\beta$. Except on very simple problems, almost always works better than AdaGrad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam and Nadam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adaptive moment estimation* combines momentum optimization and RMSProp: like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and like RMSProp, it keeps track of an exponentially decaying average of past squared gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*AdaMax*: Adam scales down the parameter updates by the $l_{2}$ norm of the time-decayed gradients (recall that the $l_{2}$ norm is the square root of the sum of squares). AdaMax replaces the $l_{2}$ norm with the  $l_{∞}$ norm (fancy way of saying the max)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nadam*: Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Power scheduling\n",
    "- Exponential scheduling\n",
    "- Piecewise constant scheduling\n",
    "- Performance scheduling\n",
    "- 1cycle scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power scheduling. `decay` is inverse of *s* (number of steps it takes to divide the learning rate by one more unit), Keras assumes *c* is equal to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential and piecewise constant scheduling require defining a function that takes the current epoch and returns the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to avoid hardcoding $\\eta_{0}$ and *s*, create a function that returns a configured function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.01**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `LearningRateScheduler` callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule function can optionally take the current learning rate as a second argument. For example, the following schedule function multiplies the previous learning rate by $0.1^{1/20}$ which results in the same exponential decay (except the decay now starts at the beginning of the epoch 0 instead of 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relies on the optimizer's initial learning rate. Make sure to set it appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer and learning rate get saved when saving a model. `epoch` argument of schedule function does not. It gets reset to 0 every time you call `fit()`. Manually set the `fit()` method's `initial_epoch` argument so the `epoch` starts at the right value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For piecewise constant scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performance scheduling, use `ReduceLROnPlateau` callback. E.g., if you pass the following callback to the `fit()` method, it will multiply the learning rate by 0.5 whenever the best validation loss does not improve for five consecutive epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras offers an alternative to implement learning rate scheduling: define the learning rate using one of the schedules available in `keras.optimizers.schedules`, then pass this learning rate to any optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: exponential decay, peformance, and 1cycle schedling can considerably speed up convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already looked at Early Stopping and Batch Normalization (designed to solve unstable gradients problems but also acts like a pretty good regularizer). Let's look at more:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ℓ_{1}$ and $ℓ_{2}$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use $ℓ_{2}$ regularization to constrain a neural network's connection weights, and/or $ℓ_{1}$ regularization if you want a sparse model (with many weights equal to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                            kernel_initializer='he_normal',\n",
    "                            kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`l2()` returns a regularizer that will be called at each step during training to compute the regularization loss. This is then added to the final loss. Use `keras.regularizers.l1()` if you want $ℓ_{1}$ regularization; for both use `keras.regularizers.l1_l2()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeating the same initialization strategy in all hidden layer, use loops or Python's `functools.partial()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                            activation='elu',\n",
    "                            kernel_initializer='he_normal',\n",
    "                            kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax',\n",
    "                        kernel_initializer='glorot_uniform')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dropout* is one of the most popular regularization techniques for deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every training setp, every neuron (including the input neurons, but always excluding the output neurons) has a probability *p* of being temporarily \"dropped out\", meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyperparameter *p* is called the *dropout rate*, and is typically set between 10% and 50%: closer to 20-30% in RNNs, and closer to 40-50% in CNNs. After training, neurons don't get dropped anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout essentially creates a new, smaller neural network at each training step. The resulting neural network can be seen as an averaging ensemble of all these smaller networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One caveat, we must multiply each input connection weight by the *keep probability* (1-*p*) after training. Alternatively, we can divide each neuron's output by the keep probability during training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is overfitting, you can increase the dropout rate. Conversely, you should decrease it if the model underfits the training set. It can also help to increase the dropout rate for large layers, and reduce it for small ones. Moreover, many architectures only use dropout after the last hidden layer, so you may want to try this if full dropout is too strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo (MC) Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                    for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model(X)` is similar to `model.predict(X)` except it returns a tensor rather than a NumPy array, and it supports the `training` argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make 100 predictions over the test set, and we stack them. Each call to the model returns a matrix with one row per instance and one column per class. Because there are 10,000 instances in the test set and 10 classes, this is a matrix of shape [10000, 10]. We stack 100 such matrices, so `y_probas` is an array of [100, 10000, 10]. Once we average over the first dimension (axis=0) we get `y_proba`, an array of shape [10000, 10], like we would get with a single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model contains other layers that behave in a special way during training (such as `BatchNormalization`), then you should not force training mode like we just did. Instead, replace the `Dropout` layers with the following `MCDropout` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Norm Regluarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each neuron, it constrains the weights **w** of the incoming connections such that ||**w**||$_{2}$ ≤ *r*, where *r* is the max-norm hyperparameter and ||.||$_{2}$ is the $ℓ_{2}$ norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not add a regularization loss term to the overall loss function. Instead, it computes ||**w**||$_{2}$ after each training step and rescaling **w** if needed <br> \n",
    "(**w** <- **w** *r*/||**w**||$_{2}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing *r* increases the amount of regularization and helps reduce overfitting. Can also help alleviate unstable gradients problem (if you are not already using Batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', \n",
    "                    kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Practical Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default DNN configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Hyperparameter        |Default value                              |\n",
    "|:---------------------|:------------------------------------------|\n",
    "|Kernel initializer    |He initialization                          |\n",
    "|Activation function   |ELU                                        |\n",
    "|Normalization         |None if shallow; Batch norm if deep        |\n",
    "|Regularization        |Early stopping (+$ℓ_{2}$ reg if needed)    |\n",
    "|Optimizer             |Momentum optimization (or RMSProp or Nadam)|\n",
    "|learning rate schedule|1cycle                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If network is simple stack of dense layers, use the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN configuration for self-normalizing net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Hyperparameter        |Default value                              |\n",
    "|:---------------------|:------------------------------------------|\n",
    "|Kernel initializer    |LeCun initialization                       |\n",
    "|Activation function   |SELU                                       |\n",
    "|Normalization         |None (self-normalization)                  |\n",
    "|Regularization        |Alpha dropout if needed                    |\n",
    "|Optimizer             |Momentum optimization (or RMSProp or Nadam)|\n",
    "|learning rate schedule|1cycle                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to normalize input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to reuse parts of a pertrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of ulabeled data, or use pretraining on an auxiliary task if you have a lot of labeled data for a similar task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some exceptions:\n",
    "- If you need a sparse model, you can use $ℓ_{1}$ regularization (and optionally zero out the tiny weights after training). If you need an even sparser model, you can use the Tensorflow Model Optimization Toolkit. This will break self-normalization, so you should use the default configuration in this case\n",
    "- If you need a low-latency model (one that performs lightning-fast predictions), you may need to use fewer layers, fold the Batch Normalization layers into the previous layers, and possibly use a faster activation function such as leaky ReLU or just ReLU. Having a sparse model will also help. Finally, you may want to reduce the float precision from 32 bits to 16 or even 8 bits (see \"Deploying a Model to a Mobile or Embedded Device\" on page 685). Again, check out TF-MOT. \n",
    "- If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC Dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
    "<br>\n",
    "No, all the values should be sampled independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Is it OK to initialize the bias terms to 0?**<br>\n",
    "It doesn't make a difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Name three advantages of the SELU activation function over ReLU.**<br>\n",
    "    1. self-normalize (if conditions are right)\n",
    "    2. it can take on negative values (helps solve vanishing/exploding gradients problem)\n",
    "    3. always has a nonzero derivative (avoids dying ReLUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**<br>\n",
    "- SELU: good default\n",
    "- leaky ReLU: faster training\n",
    "- ReLU: simplicity makes it a good alternative default despite it being outperformed by SELU and leaky ReLU\n",
    "- tanh: if you need to output a number between -1 and 1\n",
    "- logistic: if you need to estimate a probability\n",
    "- softmax: in the output layer to output probabilities for mutually exclusive classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **What may happen if you set the `momentum` hyperparameter too close to 1 (e.g., 0.99999) when using an `SGD` optimizer?**<br>\n",
    "Too little friction causes the algorithm to overshoot the global optimum, which will make it oscillate many times before converging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Name three ways you can produce a sparse model.**<br>\n",
    "    1. zero out tiny weights\n",
    "    2. $ℓ_{1}$ regularization\n",
    "    3. TensorFlow Model Optimization Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?**<br>\n",
    "Dropout slows down training (roughly by a factor of 2) but not inference (since it is only active during training). MC dropout slows down training and inference (slightly). You also want to run inference 10 times or more to get better predictions (slowing down inference by a factor of 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Practice training a deep neural network on the CIFAR10 image dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 2).\n",
       "Contents of stderr:\n",
       "2022-12-05 18:03:10.057293: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
       "2022-12-05 18:03:10.057335: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
       "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
       "                   [--host ADDR] [--bind_all] [--port PORT]\n",
       "                   [--reuse_port BOOL] [--load_fast {false,auto,true}]\n",
       "                   [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n",
       "                   [--grpc_creds_type {local,ssl,ssl_dev}]\n",
       "                   [--grpc_data_provider PORT] [--purge_orphaned_data BOOL]\n",
       "                   [--db URI] [--db_import] [--inspect] [--version_tb]\n",
       "                   [--tag TAG] [--event_file PATH] [--path_prefix PATH]\n",
       "                   [--window_title TEXT] [--max_reload_threads COUNT]\n",
       "                   [--reload_interval SECONDS] [--reload_task TYPE]\n",
       "                   [--reload_multifile BOOL]\n",
       "                   [--reload_multifile_inactive_secs SECONDS]\n",
       "                   [--generic_data TYPE]\n",
       "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
       "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
       "                   [--whatif-data-dir PATH]\n",
       "                   {serve,dev} ...\n",
       "tensorboard: error: unrecognized arguments: --"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('cifar_model.h5')\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 32, 32, 3), (5000, 32, 32, 3), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cifar(n_hidden=20, n_neurons=100, input_shape=[32,32,3], activation=0, kernel_initializer=0, add_bn=False, dropout=False):\n",
    "    activation_options = ['elu', 'selu']\n",
    "    kernel_init_options = ['he_normal', 'lecun_normal']\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "    for _ in range(20):\n",
    "        if add_bn:\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "        if dropout:\n",
    "            model.add(keras.layers.AlphaDropout(rate=0.2))\n",
    "        model.add(keras.layers.Dense(100, activation=activation_options[activation], kernel_initializer=kernel_init_options[kernel_initializer]))\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with the `keras.datasets.cifar10.load_data()`. The dataset is composed of 60,000 32x32-pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 16s 9ms/step - loss: 2.0665 - accuracy: 0.2513 - val_loss: 2.0676 - val_accuracy: 0.2496\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.8470 - accuracy: 0.3322 - val_loss: 1.8371 - val_accuracy: 0.3312\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.7747 - accuracy: 0.3589 - val_loss: 1.8113 - val_accuracy: 0.3494\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.7232 - accuracy: 0.3781 - val_loss: 1.7828 - val_accuracy: 0.3512\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.6882 - accuracy: 0.3958 - val_loss: 1.8031 - val_accuracy: 0.3636\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.6544 - accuracy: 0.4109 - val_loss: 1.7107 - val_accuracy: 0.3978\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6267 - accuracy: 0.4213 - val_loss: 1.8280 - val_accuracy: 0.3660\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6151 - accuracy: 0.4256 - val_loss: 1.6654 - val_accuracy: 0.4068\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.5977 - accuracy: 0.4323 - val_loss: 1.6698 - val_accuracy: 0.4016\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.5726 - accuracy: 0.4438 - val_loss: 1.6081 - val_accuracy: 0.4374\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.5575 - accuracy: 0.4431 - val_loss: 1.6494 - val_accuracy: 0.4196\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5408 - accuracy: 0.4520 - val_loss: 1.6675 - val_accuracy: 0.4258\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5344 - accuracy: 0.4580 - val_loss: 1.6412 - val_accuracy: 0.4260\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5160 - accuracy: 0.4632 - val_loss: 1.6194 - val_accuracy: 0.4326\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.8900 - accuracy: 0.2729 - val_loss: 1.8460 - val_accuracy: 0.3088\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.7328 - accuracy: 0.3542 - val_loss: 1.7428 - val_accuracy: 0.3762\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6517 - accuracy: 0.3915 - val_loss: 1.7545 - val_accuracy: 0.3702\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6042 - accuracy: 0.4211 - val_loss: 1.6882 - val_accuracy: 0.4036\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5748 - accuracy: 0.4325 - val_loss: 1.7120 - val_accuracy: 0.3834\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 55.5107 - accuracy: 0.4167 - val_loss: 1.9738 - val_accuracy: 0.2832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f006862fa20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar = build_cifar()\n",
    "cifar.fit(\n",
    "    X_train, y_train, epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 31s 18ms/step - loss: 1.9253 - accuracy: 0.3082 - val_loss: 1.7726 - val_accuracy: 0.3716\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.7259 - accuracy: 0.3804 - val_loss: 1.6478 - val_accuracy: 0.4152\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.6525 - accuracy: 0.4109 - val_loss: 1.6311 - val_accuracy: 0.4106\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.5996 - accuracy: 0.4346 - val_loss: 1.5604 - val_accuracy: 0.4456\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.5500 - accuracy: 0.4533 - val_loss: 1.5336 - val_accuracy: 0.4578\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.5199 - accuracy: 0.4674 - val_loss: 1.4864 - val_accuracy: 0.4728\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4781 - accuracy: 0.4765 - val_loss: 1.4477 - val_accuracy: 0.4854\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.4472 - accuracy: 0.4915 - val_loss: 1.4822 - val_accuracy: 0.4852\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4173 - accuracy: 0.5033 - val_loss: 1.4335 - val_accuracy: 0.4906\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3807 - accuracy: 0.5158 - val_loss: 1.4236 - val_accuracy: 0.4934\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3634 - accuracy: 0.5210 - val_loss: 1.4135 - val_accuracy: 0.5006\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3447 - accuracy: 0.5296 - val_loss: 1.4159 - val_accuracy: 0.4950\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.3183 - accuracy: 0.5368 - val_loss: 1.3985 - val_accuracy: 0.5134\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3008 - accuracy: 0.5438 - val_loss: 1.4127 - val_accuracy: 0.5004\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2795 - accuracy: 0.5508 - val_loss: 1.3930 - val_accuracy: 0.5170\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2636 - accuracy: 0.5581 - val_loss: 1.3828 - val_accuracy: 0.5118\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.2482 - accuracy: 0.5636 - val_loss: 1.3957 - val_accuracy: 0.5094\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2293 - accuracy: 0.5678 - val_loss: 1.3908 - val_accuracy: 0.5196\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2139 - accuracy: 0.5783 - val_loss: 1.3715 - val_accuracy: 0.5124\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.1974 - accuracy: 0.5817 - val_loss: 1.3860 - val_accuracy: 0.5254\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.1891 - accuracy: 0.5858 - val_loss: 1.3846 - val_accuracy: 0.5164\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1701 - accuracy: 0.5923 - val_loss: 1.4005 - val_accuracy: 0.5154\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1594 - accuracy: 0.5961 - val_loss: 1.3745 - val_accuracy: 0.5234\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1465 - accuracy: 0.6014 - val_loss: 1.3727 - val_accuracy: 0.5200\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.1345 - accuracy: 0.6051 - val_loss: 1.3864 - val_accuracy: 0.5242\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1244 - accuracy: 0.6060 - val_loss: 1.3757 - val_accuracy: 0.5206\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1082 - accuracy: 0.6112 - val_loss: 1.3816 - val_accuracy: 0.5236\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.0975 - accuracy: 0.6175 - val_loss: 1.3957 - val_accuracy: 0.5242\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.0912 - accuracy: 0.6180 - val_loss: 1.3729 - val_accuracy: 0.5282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f005e710c88>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar = build_cifar(add_bn=True)\n",
    "cifar.fit(\n",
    "    X_train, y_train, epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Try replacing Batch Normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 14s 9ms/step - loss: 1.9836 - accuracy: 0.2842 - val_loss: 2.0670 - val_accuracy: 0.2376\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.7729 - accuracy: 0.3716 - val_loss: 1.7369 - val_accuracy: 0.3832\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6844 - accuracy: 0.4004 - val_loss: 1.8218 - val_accuracy: 0.3300\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6221 - accuracy: 0.4278 - val_loss: 1.6324 - val_accuracy: 0.4318\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5686 - accuracy: 0.4474 - val_loss: 1.6670 - val_accuracy: 0.4280\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5242 - accuracy: 0.4655 - val_loss: 1.6282 - val_accuracy: 0.4346\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4975 - accuracy: 0.4792 - val_loss: 1.6549 - val_accuracy: 0.4328\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4690 - accuracy: 0.4875 - val_loss: 1.5898 - val_accuracy: 0.4494\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4274 - accuracy: 0.5009 - val_loss: 1.5824 - val_accuracy: 0.4638\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5145 - accuracy: 0.4782 - val_loss: 1.7161 - val_accuracy: 0.3828\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4892 - accuracy: 0.4726 - val_loss: 1.5684 - val_accuracy: 0.4544\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3939 - accuracy: 0.5132 - val_loss: 1.5566 - val_accuracy: 0.4578\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3428 - accuracy: 0.5323 - val_loss: 1.5502 - val_accuracy: 0.4714\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3143 - accuracy: 0.5433 - val_loss: 1.5288 - val_accuracy: 0.4680\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2991 - accuracy: 0.5471 - val_loss: 1.5020 - val_accuracy: 0.4794\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2761 - accuracy: 0.5560 - val_loss: 1.5386 - val_accuracy: 0.4798\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2676 - accuracy: 0.5601 - val_loss: 1.5456 - val_accuracy: 0.4714\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2330 - accuracy: 0.5699 - val_loss: 1.4742 - val_accuracy: 0.4936\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2140 - accuracy: 0.5804 - val_loss: 1.5392 - val_accuracy: 0.4828\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1932 - accuracy: 0.5869 - val_loss: 1.5022 - val_accuracy: 0.4800\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1746 - accuracy: 0.5956 - val_loss: 1.5741 - val_accuracy: 0.4890\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1594 - accuracy: 0.6022 - val_loss: 1.5459 - val_accuracy: 0.5004\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1345 - accuracy: 0.6084 - val_loss: 1.5874 - val_accuracy: 0.4952\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1144 - accuracy: 0.6169 - val_loss: 1.5994 - val_accuracy: 0.4848\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1039 - accuracy: 0.6211 - val_loss: 1.5623 - val_accuracy: 0.4960\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.0832 - accuracy: 0.6276 - val_loss: 1.5824 - val_accuracy: 0.4938\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.0569 - accuracy: 0.6388 - val_loss: 1.5511 - val_accuracy: 0.4986\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0420 - accuracy: 0.6428 - val_loss: 1.6175 - val_accuracy: 0.4842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7effb4052a58>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled = (X_train - X_train.mean()) / X_train.std()\n",
    "X_valid_scaled = (X_valid - X_valid.mean()) / X_valid.std()\n",
    "\n",
    "cifar = build_cifar(activation=1, kernel_initializer=1)\n",
    "cifar.fit(\n",
    "    X_train_scaled, y_train, epochs=100,\n",
    "    validation_data=(X_valid_scaled, y_valid),\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 18s 11ms/step - loss: 2.3115 - accuracy: 0.1337 - val_loss: 45.5669 - val_accuracy: 0.1638\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 2.0967 - accuracy: 0.1747 - val_loss: 108.1629 - val_accuracy: 0.1636\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 2.0482 - accuracy: 0.1966 - val_loss: 169.2590 - val_accuracy: 0.1708\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 2.0117 - accuracy: 0.2243 - val_loss: 92.6790 - val_accuracy: 0.1510\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 2.0072 - accuracy: 0.2292 - val_loss: 63.3845 - val_accuracy: 0.1938\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9812 - accuracy: 0.2390 - val_loss: 46.2804 - val_accuracy: 0.2052\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9703 - accuracy: 0.2403 - val_loss: 41.9689 - val_accuracy: 0.1510\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9509 - accuracy: 0.2493 - val_loss: 66.7009 - val_accuracy: 0.2050\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9511 - accuracy: 0.2532 - val_loss: 63.0197 - val_accuracy: 0.1120\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9454 - accuracy: 0.2556 - val_loss: 66.0293 - val_accuracy: 0.1686\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9403 - accuracy: 0.2580 - val_loss: 31.9389 - val_accuracy: 0.2066\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9315 - accuracy: 0.2589 - val_loss: 24.1416 - val_accuracy: 0.2064\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9396 - accuracy: 0.2553 - val_loss: 37.4155 - val_accuracy: 0.2098\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9249 - accuracy: 0.2626 - val_loss: 22.3983 - val_accuracy: 0.2132\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9192 - accuracy: 0.2640 - val_loss: 44.7398 - val_accuracy: 0.2050\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9120 - accuracy: 0.2659 - val_loss: 24.3064 - val_accuracy: 0.2062\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9362 - accuracy: 0.2596 - val_loss: 35.5947 - val_accuracy: 0.2086\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9289 - accuracy: 0.2567 - val_loss: 30.0852 - val_accuracy: 0.1808\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9162 - accuracy: 0.2659 - val_loss: 25.5664 - val_accuracy: 0.1798\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9250 - accuracy: 0.2590 - val_loss: 25.1052 - val_accuracy: 0.2132\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9135 - accuracy: 0.2688 - val_loss: 20.4783 - val_accuracy: 0.2138\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9059 - accuracy: 0.2719 - val_loss: 22.6027 - val_accuracy: 0.2128\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9040 - accuracy: 0.2761 - val_loss: 31.0790 - val_accuracy: 0.2140\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 2.3590 - accuracy: 0.2598 - val_loss: 35.2445 - val_accuracy: 0.1590\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9265 - accuracy: 0.2657 - val_loss: 17.4654 - val_accuracy: 0.2122\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9174 - accuracy: 0.2696 - val_loss: 18.4510 - val_accuracy: 0.1870\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9047 - accuracy: 0.2807 - val_loss: 20.2759 - val_accuracy: 0.2208\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9108 - accuracy: 0.2654 - val_loss: 19.7670 - val_accuracy: 0.2078\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9006 - accuracy: 0.2787 - val_loss: 35.9921 - val_accuracy: 0.2126\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9179 - accuracy: 0.2660 - val_loss: 17.7848 - val_accuracy: 0.2058\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9118 - accuracy: 0.2742 - val_loss: 18.5975 - val_accuracy: 0.1900\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9350 - accuracy: 0.2703 - val_loss: 27.6706 - val_accuracy: 0.1970\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9198 - accuracy: 0.2692 - val_loss: 17.4896 - val_accuracy: 0.2198\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9052 - accuracy: 0.2855 - val_loss: 9.7592 - val_accuracy: 0.2208\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.9051 - accuracy: 0.2864 - val_loss: 8.4324 - val_accuracy: 0.2048\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1240 - accuracy: 0.2476 - val_loss: 12.1336 - val_accuracy: 0.1994\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9367 - accuracy: 0.2624 - val_loss: 27.7815 - val_accuracy: 0.2044\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9191 - accuracy: 0.2736 - val_loss: 16.5745 - val_accuracy: 0.2106\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9096 - accuracy: 0.2830 - val_loss: 22.3517 - val_accuracy: 0.2008\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.9088 - accuracy: 0.2842 - val_loss: 19.7977 - val_accuracy: 0.1770\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9156 - accuracy: 0.2787 - val_loss: 17.1782 - val_accuracy: 0.1988\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9073 - accuracy: 0.2750 - val_loss: 16.3605 - val_accuracy: 0.2156\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9240 - accuracy: 0.2760 - val_loss: 39.4373 - val_accuracy: 0.1738\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 2.0097 - accuracy: 0.2355 - val_loss: 33.6270 - val_accuracy: 0.1676\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9662 - accuracy: 0.2460 - val_loss: 30.4033 - val_accuracy: 0.1652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7effac0e7f98>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar = build_cifar(dropout=True)\n",
    "cifar.fit(\n",
    "    X_train_scaled, y_train, epochs=100,\n",
    "    validation_data=(X_valid_scaled, y_valid),\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([cifar(X_valid_scaled, training=False) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2048\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "for i, instance in enumerate(y_proba):\n",
    "    if y_valid[i] == np.where(instance == max(instance)):\n",
    "        score += 1\n",
    "print(score / len(X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "741656c466d97e6cc8d57289f00cb22b14e49f976d4b6ab783fc4f7e7dcfa85a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
