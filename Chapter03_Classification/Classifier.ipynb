{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66cb20b8",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Most common supervised learning tasks are regression (predicting values) and classification (predicting classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9d82a",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "set of 70,000 small images of handwritten digits.<br><br>\n",
    "Scikit-Learn provides helper functions to download popular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.datasets import fetch_openml\n",
    ">>> mnist = fetch_openml('mnist_784', version=1)\n",
    ">>> mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fb81d",
   "metadata": {},
   "source": [
    "Datasets downloaded by Scikit-Learn have a similar dictionary structure. Convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0666f2",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> X, y = mnist[\"data\"], mnist[\"target\"]\n",
    ">>> X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a873b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb676e",
   "metadata": {},
   "source": [
    "70,000 images with 784 features. Each image is 28x28 pixels and each feature represents one pixel's intensity from 0 (white) to 255 (black). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = X.to_numpy()[0]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(some_digit_image, cmap=\"binary\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e58b2",
   "metadata": {},
   "source": [
    "Looks like a 5. label says its a 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a02615",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> y.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7002f7",
   "metadata": {},
   "source": [
    "Cast label to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3402dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import numpy as np\n",
    ">>> y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542463b1",
   "metadata": {},
   "source": [
    "MNIST is already split into training set (first 60,000) and test set (last 10,000). Training set is already shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a04a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be5780",
   "metadata": {},
   "source": [
    "## Training a Binary Classifier\n",
    "\n",
    "Try to classify only 1 digit, the number 5. Create the target vectors for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45711713",
   "metadata": {},
   "source": [
    "Start with _Stochastic Gradient Descent_ (SGD) classifier. Able to handle very large datasets efficiently because it deals with training instances independently, one at a time (good for online learning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "\n",
    "# requires randomness hence stochastic\n",
    "# set random state for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca781878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect images of 5\n",
    ">>> sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f7fa3",
   "metadata": {},
   "source": [
    "## Performance Measures\n",
    "\n",
    "Evalutating a classifier is trickier than evaluating a regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28366ec",
   "metadata": {},
   "source": [
    "### Measuring Accuracy Using Cross-Validation\n",
    "\n",
    "#### Implementing Cross-Validation\n",
    "Occasionally you will need more control over cross-validation process than Scikit-Learn's built in features can offer. The following does roughly the same as cross_val_score():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea92fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train_5[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_5[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51888e",
   "metadata": {},
   "source": [
    "__StratifiedKFold__ class performs stratified sampling to produce folds that contain a representative ratio of each class. At each iteration the code creates a clone of the classifier, trains that clone on the training folds, and makes predictions on the test fold. Then it counts the number of correct predictions and outputs the ratio of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd929173",
   "metadata": {},
   "source": [
    "Use cross_val_score() to evalutate SGDClassifier model, using K-folds cross-validation with 3 folds. K-fold cross-validation means splitting the training set into K folds, then making predictions and evalutating them on each fold using a model trained on the remaining folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a6436",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.model_selection import cross_val_score\n",
    ">>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf96a3b",
   "metadata": {},
   "source": [
    "Over 90% accuracy!! But wait, there's a catch. Let's look at the binary classifier that only checks for 1 number (5 or not 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return np.zeroes((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996a425",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> never_5_clf = Never5Classifier()\n",
    ">>> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6e144",
   "metadata": {},
   "source": [
    "Over 90% accuracy. This is because 10% of the images are 5s, so if you always guess that an image is not a 5, you will be right 90% of the time. <br><br>\n",
    "Demonstrates why accuracy is not the preferred performance measure for classifiers, especially with _skewed datasets_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16307011",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Better way to evaluate the performance of a classifier. Count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the fifth row and third column of the confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff9f6e",
   "metadata": {},
   "source": [
    "To compute, first need to have a set of predictions so that they can be compared to the actual targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421afaf",
   "metadata": {},
   "source": [
    "Like cross_val_score(), cross_val_predict() performs K-fold cross-validation but instead of returning the evaluation scores, it returns the predictions made on each test fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34477e51",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.metrics import confusion_matrix\n",
    ">>> confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e790b7",
   "metadata": {},
   "source": [
    "Each row represents an _actual class_ while each column represents a _predicted class_. <br>\n",
    "First row considers non-5 images (_negative class_): 0,0 correctly classified as non-5s (_true negatives_), 0,1 wrongly classified as 5s (_false positives_). <br>\n",
    "Second row considers the images of 5s (_positive class_): 1,0 were wrongly classified as non-5s (_false negatives_), 1,1 were correctly classified as 5s (_true positives_). <br>\n",
    "A perfect classifier would have only true positives and true negatives, so nonzero values on its main diagonal (top left, bottom right)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e7fda",
   "metadata": {},
   "source": [
    "_Precision_ of the classifier: precision = TP / (TP + FP) <br>\n",
    "TP = number of true positives, FP = number of false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c976a",
   "metadata": {},
   "source": [
    "Precision is typically used with _recall_, also called _sensitivity_ or the _true positive rate_ (TPR): ratio of positive instances that are correctly detected by the classifier <br>\n",
    "recall = TP / (TP + FN) <br>\n",
    "FN = number of false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725361b0",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03d699",
   "metadata": {},
   "source": [
    "Scikit-Learn provides functions to compute classifier metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d17bc",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.metrics import precision_score, recall_score\n",
    ">>> precision_score(y_train_5, y_train_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588fa1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> recall_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729c3bb3",
   "metadata": {},
   "source": [
    "Not as accurate as it was before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdbc7f",
   "metadata": {},
   "source": [
    "Combine precision and recall into one metric called _F1 score_ if you need a simple way to compare two classifiers. F1 score is the _harmonic mean_ of precision and recall. Harmonic mean gives much more weight to low values. Classifier will only get a high F1 score if both recall and precision are high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4592d",
   "metadata": {},
   "source": [
    "F1 = TP / (TP + ((FN + FP) / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.metrics import f1_score\n",
    ">>> f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febd364",
   "metadata": {},
   "source": [
    "F1 favors classifiers that have similar precision and recall. this is not always what you want: some cases you care more about precision, others more about recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6afe0a",
   "metadata": {},
   "source": [
    "You can't have it both ways: increasing precision reduces recall, and vice versa. This is called _precision/recall trade-off_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6919e4",
   "metadata": {},
   "source": [
    "### Precision/Recall Trade-Off\n",
    "\n",
    "SGDClassifier is based on a _decision function_. If the score is greater than a threshold, it assigns the instance to the positive class; otherwise it assigns it to the negative class. By increasing the threshold, it increases the precision but lowers recall because it increases the number of false negatives. By decreasing the threshold, it increases the recall but lowers the precision because it increases the number of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d8be0",
   "metadata": {},
   "source": [
    "Use SGDClassifier's decision_function() method to return a score for each instance, and then use any threshold ou want to make predictions based on those scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb65f51",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> y_scores = sgd_clf.decision_function([some_digit])\n",
    ">>> y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0162a",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> threshold = 0\n",
    ">>> some_digit_pred = (y_scores > threshold)\n",
    ">>> some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95d940",
   "metadata": {},
   "source": [
    "Let's raise the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c891f",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> threshold = 8000\n",
    ">>> y_some_digit_pred = (y_scores > threshold)\n",
    ">>> y_some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dad5b0",
   "metadata": {},
   "source": [
    "How to decide which threshold to use? First, use the cross_val_predict() function to get the scores of all instances in the training set, but this time specify that you want to return decision scores instead of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaef3a93",
   "metadata": {},
   "source": [
    "Use precision_recall_curve() to compute precision and recall for all possible thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966474e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.legend()\n",
    "\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8d962",
   "metadata": {},
   "source": [
    "Another way to select a good precision/recall trade-off is to plot precision direcly against recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e1e11",
   "metadata": {},
   "source": [
    "Suppose you aim for 90% precision, you can search for the lowest threshold that gives you at least 90% precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10212a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predicitons instead of calling classifier predict()\n",
    "y_train_pred_90 = (y_scores >= threshold_90_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bae476",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> precision_score(y_train_5, y_train_pred_90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861925df",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> recall_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0934c",
   "metadata": {},
   "source": [
    "Now we have a 90% precision classifier. However, a high precision classifier is not very useful when its recall is too low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f12dd",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "_Receiver operating characteristic_ (ROC) curve is another common tool used with binary classifiers. Plots the _true positive rate_ (another name for recall) against the _false positive rate_ (FPR). It is equal to 1 - the _true negative rate_ (TNR). TNR is also called _specificity_. ROC plots _sensitivity_ (recall) versus 1 - _specificity_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab6fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccae730",
   "metadata": {},
   "source": [
    "The higher the recall, the more false positives (FPR) the classifier produces. Dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73664f6",
   "metadata": {},
   "source": [
    "Compute _area under the curve_ (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.metrics import roc_auc_score\n",
    ">>> roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d00875",
   "metadata": {},
   "source": [
    "Prefer the precision/recall (PR) curve when you care more about the false positives than the false negatives. Otherwise, use the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437bc2a",
   "metadata": {},
   "source": [
    "Train a RandomForestClassifier and compare its ROC curve and ROC AUC score of those of the SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345aab81",
   "metadata": {},
   "source": [
    "roc_curve() function expects labels and scores but instead of scores you can use class probabilites. Use the positive class's probability as the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1]\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"RandomForest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a5bc6d",
   "metadata": {},
   "source": [
    "RandomForestClassifier's ROC curve looks better than the SGDClassifier (closer to the top left), so the ROC AUC score should be higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b0289",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> roc_auc_score(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14a555",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "_Mulitclass classifiers_ (_multinomial classifiers_) distinguish between more than two classes\n",
    "<br><br>\n",
    "Some algorithms (such as Logistic Regression, Random Forest, and naive Bayes classifiers) are capable of handling multiple classes natively. Others (such as SGD or Support Vector Machine classifiers) are strictly binary classifiers.\n",
    "<br><br>\n",
    "There are various strategies that you can use to perform multiclass classification with multiple binary classifiers. \n",
    "<br><br>\n",
    "_One-versus-the-rest_ (OvR) or _one-versus-all_ strategy: x binary classifiers that are trained, then when you want to classify an image, you get the decision score from each classifier for that image and select the class whose classifier outputs the highest score\n",
    "<br><br>\n",
    "_One-versus-one_ (OvO) strategy: create pairs for every class and compare them. If there are N classes, you must train N x (N-1) / 2 classifiers. Main advantage is that each classifier only needs to be trained on the part of the training set for the two classes it must distinguish.\n",
    "<br><br>\n",
    "Some algorithms (like SVM classifiers) scale poorly with the size of the training set. OvO is preferred because it is faster to train many classifiers on small training sets than to train few classifiers on large training sets. For most binary classifiers, however, OvR is preferred. Scikit-Learn automatically runs OvR or OvO depending on the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.svm import SVC\n",
    ">>> svm_clf = SVC()\n",
    ">>> svm_clf.fit(X_train, y_train) # y_train, not y_train_5\n",
    ">>> svm_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c045369",
   "metadata": {},
   "source": [
    "Scikit-Learn used OvO: trained 45 binary classifiers, got their decision scores for the image, and selected the class that won the most duels\n",
    "<br><br>\n",
    "Calling decision_function(), it returns 10 scores per instance (instead of 1). 1 score per class (the number of won duels plus or minus a small tweak to break ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d94de",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> some_digit_scores = svm_clf.decision_function([some_digit])\n",
    ">>> some_digit_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> np.argmax(some_digit_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> svm_clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c54ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> svm_clf.classes_[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646e7fa",
   "metadata": {},
   "source": [
    "To force Scikit-Learn to use OvO or OvR, use built in classes OnveVsOneClassifier or OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fce79d",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.multiclass import OneVsRestClassifier\n",
    ">>> ovr_clf = OneVsRestClassifier(SVC())\n",
    ">>> ovr_clf.fit(X_train, y_train)\n",
    ">>> ovr_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598da8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> len(ovr_clf.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284a7c4",
   "metadata": {},
   "source": [
    "Train SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35348e66",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> sgd_clf.fit(X_train, y_train)\n",
    ">>> sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dad4f",
   "metadata": {},
   "source": [
    "Used OvR under the hood: it trained 10 binary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb466222",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> sgd_clf.decision_function([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c8d58",
   "metadata": {},
   "source": [
    "Low score for class 3, so use cross-validation to evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e86b1b",
   "metadata": {},
   "source": [
    "Using a random classifier, it would get 10% accuracy. Scaling the inputs increases the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc45838f",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> scaler = StandardScaler()\n",
    ">>> X_train_scaled = scaler.fit_transform(X_train.astype(np.float(64)))\n",
    ">>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbbb1e4",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "In a real project, follow the Machine Learning project checklist (Appendix B). Explor data preparation options, try multiple models (shortlisting the best ones and fine-tuning their hyperparameters using GridSearchCV), and automate as much as possible. Here, assume we have a promising model and we want to improve it. Analyze the type of errors it makes.\n",
    "<br><br>\n",
    "First look at confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d2617b",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    ">>> conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    ">>> conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plat.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9b15d",
   "metadata": {},
   "source": [
    "Clearly see the errors the classifier makes. Rows represent actual classes, columsn represent predicted classes. Column for class 8 is bright, meaning many images get misclassified as 8s. However, row for class 8 is not that bad telling you that actual 8s in general get properly classified as 8s. \n",
    "<br><br>\n",
    "Conf matrix is telling us to reduce flase 8s. Could try to gather more training data for digits that look like 8s (but are not) so the classifier can learn to distinguish them from real 8s. Or engineer new features to help the classifier - writing an algorithm to count the number of closed loops (e.g., 8 has two, 6 has 1, 5 has none). Couldpreprocess the images to make some patterns, such as closed loops, stand out more.\n",
    "<br><br>\n",
    "Analyzing individual errors can help gain insights. Plot examples of 3s and 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_a, cl_b = 3, 5\n",
    "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.subplot(222); plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.subplot(223); plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.subplot(224); plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e45b0",
   "metadata": {},
   "source": [
    "Some errors are understandable due to poor handwriting (even a human would have trouble classifying). Others are obvious but the model still gets them wrong. This is because we used a simple SGDClassifier, which is a linear model. It assigns a weight per class to each pixel, and when it sees a new image it just sums up the wieghted pixel instensities to get a score for each class. Since 3s and 5s differ only by a few pixels, this model will easily confuse them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222b2a5",
   "metadata": {},
   "source": [
    "Main difference between 3 and 5 is the small line that joins the top line to the bottom arc. The classifier is quite sensitive to image shifting and rotation. One way to reduce the confusion would be to preprocess the images to ensure that they are well centered and not too rotated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11ef3f",
   "metadata": {},
   "source": [
    "## Multilabel Classification\n",
    "\n",
    "Until now, each instance has been asigned to one class. In some cases, you want your classifier to output multiple classes for each instance. E.g. facial recognition classifier. Train a facial recognition classifier to recognize 3 faces, Alice, Bob, and Charlie. Giving it a picture with Alice and Charlie should yield an output of [1, 0, 1] (Alice yes, Bob no, Charlie yes). A classification system that outputs multiple binary tags is called a _multilabel classification_ system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b62bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1237438",
   "metadata": {},
   "source": [
    "Creates a y_multilabel array containing 2 target labels for each digit: first indicates whether it is large (7, 8, 9), second indicates whether it is odd. Then we train a KNeighborsClassifier using the multiple target array. Now we make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90431e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> knn_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e6947",
   "metadata": {},
   "source": [
    "Many ways to evaluate multilabel classifier, and selecting the right metric depends on the project. One approach is to measure F1 score for each individual label (or any other binary classifier metric) then compute the average score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
    ">>> f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38f582",
   "metadata": {},
   "source": [
    "This assumes all labels are equally important, which may not be the case. Give more weight to classes with more representation (e.g. there's more pictures of Alice than Bob or Charlie). Give each label a weight equal to its _support_ (the number of instances with that target label). Set average=\"weighted\" in the preceding code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69adcd0",
   "metadata": {},
   "source": [
    "## Multioutput Classification\n",
    "\n",
    "A generalization of multilabel classification where each label can be multiclasses (it can have more than two possible values). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abffb7b",
   "metadata": {},
   "source": [
    "Build a system to remove noise from images. Input is a noisy digit image, and ouputs a clean digit image, represented as an array of pixel intensities. Classifier's output is multilabel (one label per pixel) and each label can have multiple values (pixel intensity ranges from 0 to 255)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578f07d",
   "metadata": {},
   "source": [
    "Create training and test set and add noise to the pixel intensities with NumPy's randint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909dd4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
    "X_train_mod = X_train + noise\n",
    "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
    "X_test_mod = X_test + noise\n",
    "y_train_mod = X_train\n",
    "y_test_mod = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26d1c6",
   "metadata": {},
   "source": [
    "Train the classifier and make it clean the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5932ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.fit(X_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([X_test_mod[some_index]])\n",
    "plot_digit(clean_di)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f4920",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730085f1",
   "metadata": {},
   "source": [
    "1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb9763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_neighbors': [3,4,5], 'weights': ['uniform', 'distance']}\n",
    "grid_search = GridSearchCV(knn_clf, param_grid)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cec989",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ae6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94fdd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
