{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4e6416",
   "metadata": {},
   "source": [
    "# Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225524f8",
   "metadata": {},
   "source": [
    "## Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d57b3",
   "metadata": {},
   "source": [
    "Suppose you have a few classifiers, each achieving about 80% accuracy. A simple way of creating a better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This is called a *hard voting* classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c585323",
   "metadata": {},
   "source": [
    "Often achieves a higher accuracy score than the best classifier in the ensemble. Even if each classifier is a *weak learner* (scores slightly better than random guessing), the ensemble can still be a *strong learner*, provided there are a sufficient number of weak learners and they are sufficiently diverse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c704f92",
   "metadata": {},
   "source": [
    "Creates 3 different classifiers and trains a voting classifier in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7bdb55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression()),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier()), (&#x27;svc&#x27;, SVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression()),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier()), (&#x27;svc&#x27;, SVC())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.15)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b101c",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate class probabilites (ie they all have a `predict_proba()` method) then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called *soft voting*. It often performs better than hard voting because it gives weight to highly confident votes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752ba08",
   "metadata": {},
   "source": [
    "## Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0c241",
   "metadata": {},
   "source": [
    "Another approach is to use the same training algorithm for every predictor and train them on different random subsets of the training data. When sampling is performed *with* replacement, this is called *bagging*. When performed *without* it is called *pasting*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d124a",
   "metadata": {},
   "source": [
    "Makes a prediction by aggregating the predictions of all predictors. Aggregation function is typically the *statistical mode* (ie, the most frequent prediciton, like a hard voting classifier) for classification, or the average for regression. Aggregation reduces both bias and variance. Generally, the net result is that the ensemble has a similar bias but a lower variancethan a single predictor trained on the original training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73e7d4",
   "metadata": {},
   "source": [
    "Predictors can be trained in parallel, via different CPU cores or servers. Similarly predictions can also be made in parallel. Makes this method very popular because it scales well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9707e",
   "metadata": {},
   "source": [
    "## Bagging and Pasting in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a7e9a",
   "metadata": {},
   "source": [
    "Following code trains an ensemble of 500 Decision Tree classifiers: each is trained on 100 training instances randomly sampled from the training set with replacement (bagging, but to use pasting set `bootstrap=False`). The n_jobs parameters tells Scikit-Learn the number of CPU cores to use for training and predicitons (-1 means use all available cores). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d9c8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, \n",
    "    max_samples=100, bootstrap=True, n_jobs=-1\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce269664",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e5c4b",
   "metadata": {},
   "source": [
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. About 63%of the training instances that are not sampled are called *out-of-bag* (oob) instances. Note they are not the same 37% for all predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d47b7f",
   "metadata": {},
   "source": [
    "A bagging ensemble can be evaluated using oob instances without the need for a separate validation set. In Scikit-Learn set `oob_score=True` when creating a `BaggingClassifier` to request an automatic oob evaluation after training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e400531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8aa702",
   "metadata": {},
   "source": [
    "High accuracy on test set according to the oob evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b105c4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74ade9",
   "metadata": {},
   "source": [
    "Close enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4dbae",
   "metadata": {},
   "source": [
    "`oob_decision_function_` variable returns the class probabilites for each training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2b17c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       ...,\n",
       "       [0.98159509, 0.01840491],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a251c",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267979f3",
   "metadata": {},
   "source": [
    "`BaggingClassifier` supports sampling the features controlled by two hyperparameters: `max_features` and `bootstrap_features`. Work the same way as `max_samples` and `bootstrap` but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddaf300",
   "metadata": {},
   "source": [
    "Useful when dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the *Random Patches* method. Keeping all training instances (setting `bootstrap=False` and `max_samples=1.0`) but sampling features (by setting `bootstrap_features=True` and/or `max_features` to a smaller value than 1.0) is called the *Random Subspaces* method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557072b",
   "metadata": {},
   "source": [
    "Sampling features results in even more predictor diversity: more bias for lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16cd02",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab95052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cc5e24",
   "metadata": {},
   "source": [
    "Random Forest introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. Results in greater tree diversity, trading higher bias for lower variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822a25d",
   "metadata": {},
   "source": [
    "Following `BaggingClassifier` is equivalent to the `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5130fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_features='auto', max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9933b98d",
   "metadata": {},
   "source": [
    "## Extra-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef4eaa",
   "metadata": {},
   "source": [
    "When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like Decision Trees do). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62a950",
   "metadata": {},
   "source": [
    "These types of random trees are called *Extremely Randomized Trees* ensemble (or *Extra-Trees*). Trades more bias for a lower variance. Also makes them much faster to train than Random Forests because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37fabb",
   "metadata": {},
   "source": [
    "`ExtraTreesClassifier` and `ExtraTreesRgressor` have the same API as `RandomForestClassifier` and `RandomForestRegressor` respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28829609",
   "metadata": {},
   "source": [
    "Its hard to tell which one is better, so they must be compared using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b322e8",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e975ee",
   "metadata": {},
   "source": [
    "Random Forests make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). It is a weighted average, where each node's weight is equal to the number of training samples that are associated with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110c873",
   "metadata": {},
   "source": [
    "Can be accessed using `feature_importance` variable. Example using iris dataset (returns importance by percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab996cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10149877140751586\n",
      "sepal width (cm) 0.021331743659590098\n",
      "petal length (cm) 0.46033505589063345\n",
      "petal width (cm) 0.41683442904226065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914642c0",
   "metadata": {},
   "source": [
    "Most important are petal width and length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016d8db",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05607ecb",
   "metadata": {},
   "source": [
    "*Boosting* refers to any Ensemble method that can combine several weak learners into a strong learner. Train predictors sequentially, each trying to correct its predecessor. Most popular methods are *AdaBoost* (short for *Adaptive Boosting*) and *Gradient Boosting*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd7a9ee",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764f659",
   "metadata": {},
   "source": [
    "One way for a new predictor to correct its predecssor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more onthe hard cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc0434",
   "metadata": {},
   "source": [
    "When training an AdaBoost classifier, the algorithm first trains a base classifier and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predicitions on the training set, updates the instance weights, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c49daa9",
   "metadata": {},
   "source": [
    "Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0979bd03",
   "metadata": {},
   "source": [
    "AdaBoost cannot be parallelized (because it is sequential). Each predictor can only be trained after the previous predictor has been trained and evaluated. Does not scale as well as bagging or pasting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227a89c",
   "metadata": {},
   "source": [
    "Trains an AdaBoost classifier based on 200 *Decision Stumps*. A Decision Stump is a Decision Tree with `max_depth=1`: a tree composed of a single decision node plus two leaf nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52fab449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa505316",
   "metadata": {},
   "source": [
    "If AdaBoost ensemble is overfitting training set, you can try reducing the number of estimators or more strongly regularizing the base estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4447b",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef95b8",
   "metadata": {},
   "source": [
    "Works similar to AdaBoost (sequentially adds predictors to an ensemble, each one correcting its predecessor). But instead of tweaking the instance weights at every iteration (AdaBoost), it tries to fit the new predictor to the *residual errors* made by the previous predictor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c9bb3",
   "metadata": {},
   "source": [
    "Example using Decision Trees: *Gradient Tree Boosting* or *Gradient Boosted Regression Trees* (GBRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d04d2",
   "metadata": {},
   "source": [
    "First fit a `DecisionTreeRegressor` to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4ff46df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834fdee",
   "metadata": {},
   "source": [
    "Next, train a second `DecisionTreeRegressor` on the residual errors made by the first predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b815420a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487a2e4",
   "metadata": {},
   "source": [
    "Then train a third regressor on the residual errors made by the second predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b13a9764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c107928",
   "metadata": {},
   "source": [
    "Now we have an ensemble of 3 trees that can make predictions on a new instance by adding up the predictions of all the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "712b739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X # assume new instances\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8a3200",
   "metadata": {},
   "source": [
    "Use `GradientBoostingRegressor`. Like the `RandomForestRegressor`, it has hyperparameters to control the growth of Decision Trees (eg `max_depth`, `min_samples_leaf`) and the ensemble training such as the number of trees (`n_estimators`). Following code creates the same ensemble as the previous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50656931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" checked><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ebd0c5",
   "metadata": {},
   "source": [
    "`learning_rate` scales the contribution of each tree. If set to a low value, more trees are needed to fit the training set, but the predictions will usually generalize better. This regularization technique is called shrinkage. Setting too low will cause overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51971b",
   "metadata": {},
   "source": [
    "To find optimal number of trees, utilize early stopping. Simple way to implement is using `staged_predict()` method: returns an iterator over the predictions made by the ensemble at each stage of training (with one trees, two trees, etc.). Following code trains a GBRT ensemble with 120 trees then measures the validation error at each stage of training to find the optimal number of trees and finally trains another GBRT ensemble using the optimal number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05175924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(max_depth=2, n_estimators=119)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" checked><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=2, n_estimators=119)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=119)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "         for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014b492",
   "metadata": {},
   "source": [
    "Possible to implement early stopping by actually stopping training early (instead of training a large number of trees first and then looking back to find the optimal number). Set `warm_start=True`, which makes Scikit-Learn keep existing trees when the `fit()` method is called, allowing incremental learning. Following code stops training when the validation error does not improve for five iterations in a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c07dfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float('inf')\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f857ca",
   "metadata": {},
   "source": [
    "`GradientBoostingRegressor` supports a `subsample` hyperparameter which specifies the fraction of training instances to be used for training each tree. If `subsample=0.25` then each tree is trained on 25% of the trianing instances, selected randomly. Trades higher bias for lower variance. Speeds up training considerably. This is called *Stochastic Gradient Boosting*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8a493",
   "metadata": {},
   "source": [
    "Optimized implementation of Gradient Boosting is available in XGBoost library (Extreme Gradient Boosting). XGBoost API is similar to Scikit-Learn's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b08f62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d42e8",
   "metadata": {},
   "source": [
    "Offers several nice features, like automatically taking care of early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3220fd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.36056\n",
      "[1]\tvalidation_0-rmse:0.26662\n",
      "[2]\tvalidation_0-rmse:0.20457\n",
      "[3]\tvalidation_0-rmse:0.16400\n",
      "[4]\tvalidation_0-rmse:0.13751\n",
      "[5]\tvalidation_0-rmse:0.12609\n",
      "[6]\tvalidation_0-rmse:0.12056\n",
      "[7]\tvalidation_0-rmse:0.11795\n",
      "[8]\tvalidation_0-rmse:0.11622\n",
      "[9]\tvalidation_0-rmse:0.11596\n",
      "[10]\tvalidation_0-rmse:0.11455\n",
      "[11]\tvalidation_0-rmse:0.11458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arturo/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/sklearn.py:793: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train,\n",
    "           eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5ac4e",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e4974c",
   "metadata": {},
   "source": [
    "Instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don't we train a model to perform this aggregation? When a new instance is used, the predictors make different predictions then the final predictor (called a *blender* or *meta learner*) takes the predictions as inputs and makes a final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cdca3",
   "metadata": {},
   "source": [
    "To train a blender use a hold-out-set. First training set is split into two subsets. The first subset is used to train the predictors in the first layer. Next the first layer's predictors are used to make predictions on the second (held-out) set. this ensures that the predictions are \"clean\", since the predictors never saw these instances during training. For each instance in the hold-out set, there are x predicted values (x being the number of predictors). We can create a new training set using these predicted values as input features and keeping the target values. The blender is trained on the new training set, so it learns to predict the target value, given the first layer's predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8583f1",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088736c",
   "metadata": {},
   "source": [
    "1. **If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?**\n",
    "<br>\n",
    "No; enesemble training requires that the training data be independent of one another, therefore training on the exact same data will not work (hence why we split the training randomly between all the models)\n",
    "<br>\n",
    "*It will still work fine as long as the models are all very different*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a0cfea",
   "metadata": {},
   "source": [
    "2. **What is the difference between hard and soft voting classifiers?**\n",
    "<br>\n",
    "Hard voting aggregates the classes predicted by the predictors and picks the most chosen. Soft voting averages the probability of each class chosen by the predictors and returns the highest probability (requires `predict_proba()` function in every predictor used)\n",
    "<br>\n",
    "*Soft voting gives high confidence votes more weight*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b426b1",
   "metadata": {},
   "source": [
    "3. **Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, Random Forests, or stacking ensembles?**\n",
    "Training can be done across multiple cores/servers when using bagging, pasting, and Random Forests (due to the nature of training several predictors independently and comparing them at the end). However, with boosting and stacking, training each predictor is not independent of one another. Each predictor is created with some output from a predecessor (whether it be error, layers, etc). This means training must be done sequentially and not in parallel.\n",
    "<br>\n",
    "*Each predictor in boosting ensemble is built based on the previous predictor (error), so training is necessarily sequential. For stacking, all the predictors in a given layer are independent of each other, so they can be trained in parallel on multiple servers. However, the predictors in one layer can only be trained after the predictors in the previous layer have all been trained.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73727ca",
   "metadata": {},
   "source": [
    "4. **What is a benefit of out-of-bag evaluation?**\n",
    "<br>\n",
    "It yields a reasonable estimation\n",
    "<br>\n",
    "*Evaluated using instances that were not trained on (held out). This makes it possible to have a fairly unbiased evaluation of the ensemble without the need for a validation set*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c880b3",
   "metadata": {},
   "source": [
    "5. **What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?**\n",
    "<br>\n",
    "Only a random subset of features is considered for splitting. They can also use random thresholds per feature instead of finding the best threshold (like a Decision Tree does). This is the longest part of training a Decision Tree so it speeds up training as well. \n",
    "<br>\n",
    "*Randomized thresholds act like an extra step of regularization; better if Randomized Forest is overfitting data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40077c79",
   "metadata": {},
   "source": [
    "6. **If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak and how?**\n",
    "<br>\n",
    "Try increasing the number of estimators or strongly regularizing the base estimator.\n",
    "<br>\n",
    "*Also slightly increase the learning rate*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffad08",
   "metadata": {},
   "source": [
    "7. **If your Gradient Boosting ensemble overfits the training data, should you increase or decrease the learning rate?** <br>\n",
    "Decrease the learning rate (create more bias generalizing predictions). \n",
    "<br>\n",
    "*Use early stopping to find the right number of predictors (probably have too many)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb48a4",
   "metadata": {},
   "source": [
    "8. **Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g, use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM classifier. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5a324",
   "metadata": {},
   "source": [
    "Load data and split into training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d4b977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n",
    "X_train, X_test, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=10000, random_state=42)\n",
    "\n",
    "# X_train, X_val, X_test = X[:50000], X[50000:60000], X[60000:]\n",
    "# y_train, y_val, y_test = y[:50000], y[50000:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a81f1",
   "metadata": {},
   "source": [
    "Training various classifiers (Random Forest, Extra-Trees, SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4bde3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "ext_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = LinearSVM(random_state=42)\n",
    "#mlp_clf = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [rnd_clf, ext_clf, svm_clf]\n",
    "for estimator in estimators:\n",
    "    print('Training', estimator)\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[estimator.score(X_val, y_val) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2ec00",
   "metadata": {},
   "source": [
    "**Voting classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rnd_clf), ('et', ext_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc85b1d",
   "metadata": {},
   "source": [
    "Accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86848d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "[estimator.score(X_test, y_test) for estimator in estimators] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed0d20",
   "metadata": {},
   "source": [
    "9. **Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image's class. Train a classifier on this new training set. Congratulations, you have just trained a blender, and together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble's predictions. How does it compare to the voting classifier you trained earlier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bcc2a2",
   "metadata": {},
   "source": [
    "Create new training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533af41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pred = rnd_clf.predict(X_val)\n",
    "ext_pred = ext_clf.predict(X_val)\n",
    "#mlp_pred = mlp_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = []\n",
    "for i, j, k in zip(rnd_pred, ext_pred):\n",
    "    X_new.append([i, j, k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb74684",
   "metadata": {},
   "source": [
    "Train new classifier on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "blender = RandomForestClassifier()\n",
    "blender.fit(X_new, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5b74b",
   "metadata": {},
   "source": [
    "Make predictions with stacking ensemble (using predictions from previous question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a09874",
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_set = []\n",
    "for i in range(len(X_test)):\n",
    "    r, e, s = y1_pred[i], y2_pred[i], y3_pred[i]\n",
    "    blend_set.append([r, e, s])\n",
    "\n",
    "blend_predict = blender.predict(blend_set)\n",
    "accuracy_score(blend_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb5586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
