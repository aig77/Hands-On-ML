{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45516633",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b01a8",
   "metadata": {},
   "source": [
    "## Training and Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc4e501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700817b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_graphviz\n\u001b[1;32m      3\u001b[0m export_graphviz(\n\u001b[1;32m      4\u001b[0m     tree_clf,\n\u001b[0;32m----> 5\u001b[0m     out_file\u001b[38;5;241m=\u001b[39m\u001b[43mimage_path\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miris_tree.dot\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39miris\u001b[38;5;241m.\u001b[39mfeature_names[\u001b[38;5;241m2\u001b[39m:],\n\u001b[1;32m      7\u001b[0m     class_names\u001b[38;5;241m=\u001b[39miris\u001b[38;5;241m.\u001b[39mtarget_names,\n\u001b[1;32m      8\u001b[0m     rounded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_path' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=image_path(\"iris_tree.dot\"),\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f884a",
   "metadata": {},
   "source": [
    "`$ dot -Tpng iris_tree.dot -o iris_tree.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc71e7",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6ac61",
   "metadata": {},
   "source": [
    "`samples` attribute counts how many training instances it applies to <br>\n",
    "`value` attribute tells you how many training instances of each class this node applies to <br>\n",
    "`gini` attribute measures its *impurity* (\"pure\" if gini=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ce2c9",
   "metadata": {},
   "source": [
    "Gini impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e077927",
   "metadata": {},
   "source": [
    "$G_{i} = 1 - \\sum_{k=1}^{n}p_{i,k}^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf3a8e",
   "metadata": {},
   "source": [
    "$p_{i,k}$ is the ratio of class k instances among the training instances in the ith node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ffd4ac",
   "metadata": {},
   "source": [
    "### Model Interpretation: White Box vs Black Box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a971bf7",
   "metadata": {},
   "source": [
    "Decision Trees are intuitive and their decisions are easy to interpret. Such models are often called *white box models*. In contrast, Random Forests or neural networks are considered *black box models*. They make great predictions and the calculations they perform are easy to check, but it is hard to explain why the predictioins were made. Conversely, Decision Trees provide nice, simple classification rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd63b8",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3a881",
   "metadata": {},
   "source": [
    "Decision trees also estimate the probability that an instance belongs to a particualr class k by returning the ratio of training instances of class k in a particular node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29757b1",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3ae40",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594d987",
   "metadata": {},
   "source": [
    "Scikit-Learn uses the *Classification and Regression Tree* (CART) algorithm to train Decision Trees (also called \"growing\" trees). Algorithm works by first splitting the training set into two subsets using a single feature $k$ and a threshold $t_{k}$. It finds $t$ and $t_{k}$ by searching for the pair that produces the purest subsets (weighted by their size). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f69053",
   "metadata": {},
   "source": [
    "CART cost function for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75052fb5",
   "metadata": {},
   "source": [
    "$J(t,t_{k}) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21462d68",
   "metadata": {},
   "source": [
    "Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth (defined by `max_depth`) or if it cannot find a split that will reduce impurity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5179b4",
   "metadata": {},
   "source": [
    "CART is a greedy algorithm. Most of the time, it will not produce a solution with the lowest possible impurity (not guranteed to be optimal). This is because finding the optimal tree is considered an NP-Complete problem. Therefore, we must settle for a reasonably good solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96769c12",
   "metadata": {},
   "source": [
    "## Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27030f7",
   "metadata": {},
   "source": [
    "Making predictions required traversing the Decision Tree from the root to a leaf. Decision Trees are roughly balanced, so runtime is roughly $O(log_{2}(m))$. Since each node only requires checking the value of one feature, the overall prediction complexity is $O(log_{2}(m))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55818d52",
   "metadata": {},
   "source": [
    "Training compares all features which is $O(n*mlog_{2}(m))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b7554",
   "metadata": {},
   "source": [
    "## Gini Impurity or Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93e078",
   "metadata": {},
   "source": [
    "In ML, *entropy* is often used as an impurity measure after Shannon's *information theory*, where it measures the average information content of a message: entropy is zero when al messages are identical. By default Gini impurity measure is used. To switch to entropy, set `criterion` hyperparameter to `\"entropy\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793df281",
   "metadata": {},
   "source": [
    "Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6848f4",
   "metadata": {},
   "source": [
    "$H_{i} = - \\sum_{k=1}^n p_{i,k} log_{2}(p_{i,k})$\n",
    "<br>\n",
    "such that $p_{i,k}$ != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a98ee",
   "metadata": {},
   "source": [
    "Gini is slightly faster to. When they differ, Gini tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1478cf",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491dbbb2",
   "metadata": {},
   "source": [
    "Decision Trees make very few assumptions about training data, so they are likely to fit it every well. This can lead to overfitting. This type of model is called a *nonparametric model* because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. In contrast, a *parametric model*, such as a linear model, has predetermined number of parameters, so its degree of freedom is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a519060",
   "metadata": {},
   "source": [
    "Restrict Decision Tree's freedom during druing to avoid overfitting (regularization). In Scikit-Learn, reduce `max_depth` hyperparamter to regularize the model and reduce risk of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff394452",
   "metadata": {},
   "source": [
    "`DecisionTreeClassifier` class has other parameters that restrict the shape of the Decision Tree: <br>\n",
    "`min_samples_leaf` - the minimum number of samples a leaf node must have) <br>\n",
    "`min_weight_fraction_leaf` - the same as `min_samples_leaf` but expressed as a fraction of the total number of weighted instances <br>\n",
    "`max_leaf_nodes` - the maximum number of leaf nodes <br>\n",
    "`max_features` - the maximum number of features that are evalutated for splitting at each node <br>\n",
    "Increasing `min_*` hyperparameters or reducing `max_*` hyperparameters will regularize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554412e",
   "metadata": {},
   "source": [
    "Other algorithms work by first training Decision Tree without restrictions, then *pruning* (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant. Use $\\chi^{2}$ test (chi-squared test) to estimate the probability that the improvement is purely the result of chance (null hypothesis). If the probability is higher than the set *p-value*, a given threshold, then the node is considered unnecessary and its children are deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6da341",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704687e",
   "metadata": {},
   "source": [
    "`DecisionTreeRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015dc187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8568a28e",
   "metadata": {},
   "source": [
    "Instead of predicting a class in each node, it predicts a value - the average target value of the m training instances associated with this leaf node, and it results in a mean squared error over the instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f07e2",
   "metadata": {},
   "source": [
    "CART works the same way, but instead of trying to split the training set in a way that minimizes impurity, it tries to split the training set in a way that minimizes the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9e4e9",
   "metadata": {},
   "source": [
    "CART cost function for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31dbb9",
   "metadata": {},
   "source": [
    "$J(k,t_{k}) = \\frac{m_{left}}{m}MSE_{left} + \\frac{m_{right}}{m}MSE_{right}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62926f3a",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc38b60",
   "metadata": {},
   "source": [
    "$MSE_{node} = \\sum_{i ∈ node} (\\hat{y}_{node} - y^{(i)})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e8d37",
   "metadata": {},
   "source": [
    "$\\hat{y} = \\frac{1}{m_{node}} \\sum_{i ∈ node} y^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c5f67",
   "metadata": {},
   "source": [
    "Just like classification, decision trees for regression are prone to overfitting without regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbef09f",
   "metadata": {},
   "source": [
    "## Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17654038",
   "metadata": {},
   "source": [
    "Decision Trees are powerful, but have some limitations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c63fc",
   "metadata": {},
   "source": [
    "Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis); this makes them sensitive to training set rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4377b7ad",
   "metadata": {},
   "source": [
    "Biggest weakness is that they're very sensitive to small variations in the training data. For example, removing the widest *Iris versicolor* from the iris training set and training a new Decision Tree, gives you a very different model. Random Forests limit this instability by averaging predictions over many trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3dff9",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2935bc23",
   "metadata": {},
   "source": [
    "### 1. What is the approximate depth of a Decision Tree training (without retstrictions) on a training set with one million instances?\n",
    "<br>\n",
    "$log_{2}$(1 million)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378fffe",
   "metadata": {},
   "source": [
    "### 2. Is a node's Gini impurity generally lower or greater than its parent's? Is it *generally* lower/greater, or *always* lower/greater?\n",
    "<br>\n",
    "It is generally lower. Because growing a tree is an NP complete problem, we settle for using a Greedy algorithm that attempts to minimize the gini at each possible depth. This doesn't always happen (because of the greedy approach is not optimal) so it may get stuck at a local minimum before breaking out and finding the true minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e0acc",
   "metadata": {},
   "source": [
    "### 3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing `max_depth`?\n",
    "<br>\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b12ec4",
   "metadata": {},
   "source": [
    "### 4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
    "<br>\n",
    "Scaling is always a good idea regardless of under or overfitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23650ec",
   "metadata": {},
   "source": [
    "### 5. If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances?\n",
    "<br>\n",
    "Training is $O(n*mlog_{2}(m))$ so assume n=1, 1 x 1 $log_{2}$(1) is roughly an hour,<br>\n",
    "then 1 x 10 $log_{2}$(10) = about 30 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47884ed",
   "metadata": {},
   "source": [
    "### 6. If your training set contains 100,000 instances, will setting `presort=True` speed up training?\n",
    "<br>\n",
    "That may be small enough to speed up training. If the training set was larger, it would probably slow it down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c767e6",
   "metadata": {},
   "source": [
    "### 7. Train and fine-tune a Decision Tree for the moons dataset by following the following steps:\n",
    "a. Use `make_moon(n_samples=10000, noise=0.4)` to generate a moons dataset <br>\n",
    "b. Use `train_test_split()` to split the dataset into a training set and a test set <br>\n",
    "c. Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a `DecisionTreeClassifier`. Hint: try various values for `max_leaf_nodes` <br>\n",
    "d. Train it on the full training set using these hyperparameters, and measure your model's performance on the test set. You should roughly get 85% to 87% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694a0390",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'make_moon' from 'sklearn.datasets' (/Users/arturo/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_moon\n\u001b[1;32m      3\u001b[0m X, y \u001b[38;5;241m=\u001b[39m make_moon(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'make_moon' from 'sklearn.datasets' (/Users/arturo/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moon\n",
    "\n",
    "X, y = make_moon(n_samples=10000, noise=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc67b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
