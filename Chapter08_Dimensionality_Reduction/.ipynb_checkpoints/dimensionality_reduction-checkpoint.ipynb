{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba7354a",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e79ecab",
   "metadata": {},
   "source": [
    "*Curse of dimensionality*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1a414",
   "metadata": {},
   "source": [
    "Reducing dimensionality does cause some information loss\n",
    "- speeds up training\n",
    "- may make system perform slightly worse\n",
    "- makes pipelines a bit more complex and harder to maintain\n",
    "- may filter out some noise and unnecessary details in some cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8648f47",
   "metadata": {},
   "source": [
    "Useful for data visualization (*DataViz*); reducing high dimensional data to 2D or 3D makes it possible to plot a condensed view of a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns such as clusters\n",
    "<br>\n",
    "Also useful for communicating conclusions to people who are not data scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d46d8",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0542109",
   "metadata": {},
   "source": [
    "Picking a random point in a unit square (1x1 square) will have about a 0.4% chance of being located less than 0.001 from a border (ie it is very unlikely that a random point will be \"extreme\" along any dimension). But in a 10,000-dimensional unit hypercube, this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very close to the border."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e3b9a6",
   "metadata": {},
   "source": [
    "If you pick two points randomly in a unit square, the distance between these two points will be, on average, roughly 0.52. If you pick two random points in a 3D cube, the average distance will be roughly 0.66. If two points are picked randomly in a 1,000,000-dimensional hypercube the average distance is about 408.25 ($\\sqrt{10^{6}/6}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4346d3d",
   "metadata": {},
   "source": [
    "**There is plently of space in high dimensions**. As a result, high-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. This means new instances are likely to be far away from any training instance, making predictions less reliable. More dimensions = greater risk of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb365a",
   "metadata": {},
   "source": [
    "## Main Approaches for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57eff3",
   "metadata": {},
   "source": [
    "### Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e74a975",
   "metadata": {},
   "source": [
    "In most real-world problems, training instances are *not* spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances lie within (or close to) a much lower-dimensional *subspace* of the high-dimensional space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484117a",
   "metadata": {},
   "source": [
    "Projecting high-dimensional data (like points in a 3D space) to a low-dimension subspace (like a plane) can help reduce dimensionality. This, however, is not always useful. Take the *Swiss roll* example. Projecting it to 2D directly would squash the points of the swiss roll together causing overlapping points. Instead, unroll the Swiss roll to spread the points into 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8954eaf6",
   "metadata": {},
   "source": [
    "### Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21046db0",
   "metadata": {},
   "source": [
    "Swiss roll is an example of a 2D *manifold*. A 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. More generally, a d-dimensional manifold is. part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94d2fa",
   "metadata": {},
   "source": [
    "Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; called *Manifold Learning*. It relies on the *manifold assumption*, or *manifold hypothesis*, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf635b6c",
   "metadata": {},
   "source": [
    "The manifold assumption is often accompanied by another implicit assumption: that the task at hand (classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold. However, this assumption *does not* always hold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd96d7",
   "metadata": {},
   "source": [
    "In short, reducing the dimensionality of your training set before training a model will usually speed up training, but not always lead to a better or simpler solution; it depends on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2bad4a",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a2136",
   "metadata": {},
   "source": [
    "*Priniciple Component Analysis* is the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, then it projects the data onto it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a324c",
   "metadata": {},
   "source": [
    "### Preserving the Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e8eba",
   "metadata": {},
   "source": [
    "Before projecting the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane. There may be several in choices in the same dimension. It is reasonable to select the hyperplane that preserves the maximum amount of variance, as it will likely lose less information than the other projections. Another way to justify this choice is that it is the hyperplane that minimizes the mean squared distance between the original dataset and its projection on that hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cc1e8",
   "metadata": {},
   "source": [
    "### Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ef290",
   "metadata": {},
   "source": [
    "PCA identifies the axis that accounts for the largest amount of variance in the training set. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In higher-dimensional data, it continues finding these axes (as many as the number of dimensions in the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb8547",
   "metadata": {},
   "source": [
    "The *$i^{th}$* axis is called the *$i^{th}$ principal component* (PC) of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea66b37",
   "metadata": {},
   "source": [
    "How to find the PC of a training set? Use *Singular Value Decomposition*, a standard matrix factorization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8978f",
   "metadata": {},
   "source": [
    "Following code uses NumPy's `svd()` function to obtain all the principal components of the training set, then extracts the two unit vectors that define the first two PCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea04a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976f6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78761347",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c45031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_centered = X_sample - X_sample.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab88023",
   "metadata": {},
   "source": [
    "PCA assumes that the dataset is centered around the origin. Scikit-Learn's PCA classes take care of centering the data. If implementing PCA or using other libraries remember to center data first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456f159",
   "metadata": {},
   "source": [
    "### Projecting Down to d Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd89d78",
   "metadata": {},
   "source": [
    "Once all the principal components have been identified, reduce the dimensionality of the dataset down to *d* dimensions by projecting it onto the hyperplane defined by the first *d* principal components. This will preserve as much variance as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35f25b",
   "metadata": {},
   "source": [
    "Following code projects the training set onto the plane defined by the first two principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5311810",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aab31e",
   "metadata": {},
   "source": [
    "### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee91abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f34b46",
   "metadata": {},
   "source": [
    "After fitting the PCA transformer to the dataset, it `components_` attribute holds the transpose of the matrix containing the first *d* columns of **V** (contains all the unit vectors that define all the principle components that we are looking for)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4f255",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe942a",
   "metadata": {},
   "source": [
    "*Explained Variance Ratio* of each principal component is available via `explained_variance_ratio_` variable. It indicated the proportion of the dataset's variance that lies along each principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5a161",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd85857",
   "metadata": {},
   "source": [
    "Instead of choosing randomly, choose the number of dimensions that add up to a sufficiently large proportion of the variance (eg 95%); unless you are reducing for DataViz (reduce to 2 or 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2ffca",
   "metadata": {},
   "source": [
    "Performs PCA without reducing dimensionality, then computes the min number of dimensions required to preserve 95% of the training set's variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6453e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_sample)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8648ed",
   "metadata": {},
   "source": [
    "Then set `n_components=d` and run PCA again. Or you can set `n_components` to be a float between 0.0 and 1.0 indicating the ratio of variance you wish to preserve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a17866e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb3b6c",
   "metadata": {},
   "source": [
    "### PCA for Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1adf128",
   "metadata": {},
   "source": [
    "After dimensionality reduction the training set takes up much less space. With MNIST, should have just over 150 features instead of the original 784. While most of the variance is preserved, the dataset is now less than 20% of its original size! This is a reasonable compression ratio and should speed up training a classifier (such as SVM) tremendously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2f883",
   "metadata": {},
   "source": [
    "It is possible to decompress the data back to its original dimensions by applying the inverse transformation of the PCA projection. This won't give you back the original data (5% that was dropped) but will likely be close to the original. The mean squared distance between the original data and the reconstructged data (compressed then decompressed) is called the *reconstruction error*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aa0abd",
   "metadata": {},
   "source": [
    "Following compresses the MNIST down to 154 dimensions then uses `inverse_transform()` to decompress back to 784:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cbd7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d1637",
   "metadata": {},
   "source": [
    "### Randomized PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4400e1",
   "metadata": {},
   "source": [
    "Setting hyperparameter `svd_solver=\"randomized\"`, Scikit-Learn uses stochastic algorithm called *Randomized PCA* to find an approximation of the first *d* principal components. Its computational complexity is $O(m*d^{2})+O(d^{3})$ instead of $O(m*n^{2})+O(n^{3})$, so it is faster when d < n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68194398",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver='randomized')\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd134886",
   "metadata": {},
   "source": [
    "By default, `svd_solver='auto'`: Scikit-Learn uses the randomized PCA if m or n is greater than 500 and d is less than 80% of m or n, else it uses full SVD approach. To use full SVD set `svd_solver='full'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8195b9",
   "metadata": {},
   "source": [
    "### Incremental PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d9c4f",
   "metadata": {},
   "source": [
    "*Incremental PCA* (IPCA) allow you to split the training set into mini-batchjes and feed an IPCA algorithm one mini-batch at a time. Useful if training set is large and doesn't fit in memory or applying PCA online (ie on the fly as new instances arrive). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2cf89a",
   "metadata": {},
   "source": [
    "Splits MNIST into 100 mini-batches (using NumPy's `array_split()` function) and feeds them to Scikit-Learn's `IncrementalPCA` class to reduce the dimensionality of MNIST to 154 like before. Note that you must call `partial_fit()` with each mini-batch, rather than the `fit()` method on the whole training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2474c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315cd76",
   "metadata": {},
   "source": [
    "Alternatively, use NumPy's `memmap` class, which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory; the class loads only the data it needs in memory, when it needs it. Makes it possible to call the usual `fit()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "811607df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_mm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmemmap(\u001b[43mfilename\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadonly\u001b[39m\u001b[38;5;124m'\u001b[39m, shape\u001b[38;5;241m=\u001b[39m(m,n))\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_batches\n\u001b[1;32m      4\u001b[0m inc_pca \u001b[38;5;241m=\u001b[39m IncrementalPCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m154\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "X_mm = np.memmap(filename, dtype='float32', mode='readonly', shape=(m,n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c9974a",
   "metadata": {},
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c13f5",
   "metadata": {},
   "source": [
    "Recall that a linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the *original space*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87b823",
   "metadata": {},
   "source": [
    "*Kernel PCA* is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf67c5d",
   "metadata": {},
   "source": [
    "### Selecting a Kernel and Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d170d021",
   "metadata": {},
   "source": [
    "kPCA is an unsupervised learning algorithm, there are no obvious performance measures to help you select the best kernel and hyperparameter values. That said, dimensionality reduction is often a preparation step for a supervised learning task (eg classification), so you can use grid search to select the kernel and hyperparameters that lead to the best performance on that task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293fca7d",
   "metadata": {},
   "source": [
    "Creates a two-step pipeline, first reducing dimensionality to two dimensions using kPCA, then applying Logistic Regression for classification. Then it uses `GridSearchCV` to find the best kernel and gamma value for kPCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9318d",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b16339",
   "metadata": {},
   "source": [
    "Another approach (unsupervised) is to select the kernel and hyperparameters that yield the lowest reconstruction error. Note reconstruction is not always as easy as with linear PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d59cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433,\n",
    "                   fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e4359",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab25918c",
   "metadata": {},
   "source": [
    "Now use grid search with cross-validation to find the kernel and hyperparameters that minimize the error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f47472",
   "metadata": {},
   "source": [
    "## LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022c0fa",
   "metadata": {},
   "source": [
    "*Locally Linear Embedding* (LLE) is another powerful *nonlinear dimensionality reduction* (NLDR) technique. It is a Manifold Learning technique that does not rely on projections (like the previous algorithms do). It works by first measuring how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved. Makes it good at unrolling twisted manifolds, especially when there is not too much noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd995038",
   "metadata": {},
   "source": [
    "Scikit-Learn implementation has $O(mlog(m)nlog(k))$ computational complexity for finding the k nearest neighbors, $O(mnk^{3})$ for optimizing the weights, and $O(dm^{2})$ for constructing the low-dimensional representations. Unfortunately, the last term makes this algorithm scale poorly to very large datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b241fe3",
   "metadata": {},
   "source": [
    "## Other Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a3438",
   "metadata": {},
   "source": [
    "*Random Projections*: projects the data to a lower-dimensional space using a random linear projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af904e66",
   "metadata": {},
   "source": [
    "*Multidimensional Scaling* (MDS): Reduces dimensionality while trying to preserve the distance between the instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddaebf8",
   "metadata": {},
   "source": [
    "*Isomap*: creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the *geodesic distances* between the instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d194a",
   "metadata": {},
   "source": [
    "*t-Distributed Stochastic Neighbor Embedding* (t-SNE): reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space (eg, to visualize the MNIST images in 2D). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc2e5f4",
   "metadata": {},
   "source": [
    "*Linear Discriminant Analysis* (LDA): Is a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit of this approach is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf37fb",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959af37d",
   "metadata": {},
   "source": [
    "1. **What are the main motivations for reducing a dataset's dimensionality? What are the main drawbacks?**\n",
    "<br>\n",
    "Reducing a dataset's dimensionality shrinks the dataset making it faster to train. It is also possible to keep a majority of the data's importance while shrinking it significantly. The downside is that some of the data's information is lost creating, more bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80e2c1",
   "metadata": {},
   "source": [
    "2. **What is the curse of dimensionality?**\n",
    "<br>\n",
    "Instances within a high dimensional dataset are further apart from one another than they are in lower dimensions. This makes high-dimensional datasets very sparse and can lead to less reliable predictions than low-dimensional data. In short, the more dimensions the training set has, the greater risk of overfitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b899c423",
   "metadata": {},
   "source": [
    "3. **Once a dataset's dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?**\n",
    "<br>\n",
    "The operation can be reversed using a technique called reconstruction where the data is reprojected into higher dimensions and an algorithm will fill in missing points based on the existing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc522c2",
   "metadata": {},
   "source": [
    "4. **Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?**\n",
    "<br>\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c3c06",
   "metadata": {},
   "source": [
    "5. **Suppose you perform PCA on a 1000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?**\n",
    "<br>\n",
    "It will reduce it to the smallest amount of dimensions that still retains 95% explained variance ratio (the ratio indicates the proportion of the dataset's variance that lies along each principal component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea530c",
   "metadata": {},
   "source": [
    "6. **In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?**\n",
    "<br>\n",
    "Vanilla works well on a dataset that is not too large (fits in memory). Incremental works well on datasets that do not fit in memory. Randomized works well if training takes too long with the other methods (it will speed up training), and Kernel PCA works well on smaller datasets (."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a3252",
   "metadata": {},
   "source": [
    "7. **How can you evalutate the performance of a dimensionality reduction algorithm on your dataset?**\n",
    "<br>\n",
    "Compare the results to training without dimensionality reduction. Most times, it should improve accuracy slightly (assuming the dataset has many dimensions). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094a5f76",
   "metadata": {},
   "source": [
    "8. **Does it make any sense to chain two different dimensionality reduction algorithms?**\n",
    "<br>\n",
    "No, most dimensionality reduction algorithms are designed to maximze the variance in low-dimensional data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2123489",
   "metadata": {},
   "source": [
    "9. **Load the MNIST dataset and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next, evaluate the classifier on the test set. How does it compare to the previous classifier?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4468bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f02296a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52fc4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist[\"data\"][60000:], mnist[\"target\"][60000:]\n",
    "X_test, y_test = mnist[\"data\"][:60000], mnist[\"target\"][:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f8405f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bb1868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "rf1.fit(X_train, y_train)\n",
    "time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1685ab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No dimensionality reduction: 3.522034168243408 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"No dimensionality reduction:\", time2-time1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a4abcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94505"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf1_pred = rf1.predict(X_test)\n",
    "accuracy_score(rf1_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb120d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6211a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eee786a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time3 = time.time()\n",
    "rf2.fit(X_train_reduced, y_train)\n",
    "time4 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99544a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA evr=0.95: 8.372006177902222 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"PCA evr=0.95:\", time4-time3, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bfa97bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reduced = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b68f2938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9170666666666667"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2_pred = rf2.predict(X_test_reduced)\n",
    "accuracy_score(rf2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754e26b",
   "metadata": {},
   "source": [
    "Using PCA is nearly 3x slower and drops accuracy by about 3% in this case. Not advisable :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97db3e7a",
   "metadata": {},
   "source": [
    "10. **Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image's target class. Alternatively, you can replace each dot in the scatterplot with the corresponding instance's class (a digit from 0 to 9), or even plot scaled-down versions of the digit images themselves (if you plot all digits, the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1fee9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_a, X_b, y_a, y_b = train_test_split(X, y, train_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a6082f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arturo/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/arturo/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m      3\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m X_reduced \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_a\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1117\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m        Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:957\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[t-SNE] Indexed \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m samples in \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124ms...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    952\u001b[0m             n_samples, duration\n\u001b[1;32m    953\u001b[0m         )\n\u001b[1;32m    954\u001b[0m     )\n\u001b[1;32m    956\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 957\u001b[0m distances_nn \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m duration \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/neighbors/_base.py:924\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors_graph\u001b[0;34m(self, X, n_neighbors, mode)\u001b[0m\n\u001b[1;32m    921\u001b[0m     A_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(n_queries \u001b[38;5;241m*\u001b[39m n_neighbors)\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 924\u001b[0m     A_data, A_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     A_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mravel(A_data)\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/neighbors/_base.py:763\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    756\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m PairwiseDistancesArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[1;32m    759\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    761\u001b[0m )\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[0;32m--> 763\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mPairwiseDistancesArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[1;32m    775\u001b[0m ):\n\u001b[1;32m    776\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[1;32m    777\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[1;32m    778\u001b[0m     )\n",
      "File \u001b[0;32msklearn/metrics/_pairwise_distances_reduction.pyx:691\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction.PairwiseDistancesArgKmin.compute\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/fixes.py:151\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[0;34m(limits, user_api)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthreadpoolctl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/threadpoolctl.py:171\u001b[0m, in \u001b[0;36mthreadpool_limits.__init__\u001b[0;34m(self, limits, user_api)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(limits, user_api)\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_threadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/threadpoolctl.py:268\u001b[0m, in \u001b[0;36mthreadpool_limits._set_threadpool_limits\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m modules \u001b[38;5;241m=\u001b[39m \u001b[43m_ThreadpoolInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                          \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# self._limits is a dict {key: num_threads} where key is either\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# a prefix or a user_api. If a module matches both, the limit\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# corresponding to the prefix is chosed.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[0;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m user_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m user_api\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/threadpoolctl.py:371\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m\"\"\"Loop through loaded libraries and store supported ones\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_modules_with_dyld\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_enum_process_module_ex()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/threadpoolctl.py:428\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_dyld\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m filepath \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# Store the module if it is supported and selected\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_module_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/threadpoolctl.py:515\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._make_module_from_path\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefixes \u001b[38;5;129;01mor\u001b[39;00m user_api \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api:\n\u001b[1;32m    514\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[module_class]\n\u001b[0;32m--> 515\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mappend(module)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/threadpoolctl.py:606\u001b[0m, in \u001b[0;36m_Module.__init__\u001b[0;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_api \u001b[38;5;241m=\u001b[39m internal_api\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(filepath, mode\u001b[38;5;241m=\u001b[39m_RTLD_NOLOAD)\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_threads()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_extra_info()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/threadpoolctl.py:646\u001b[0m, in \u001b[0;36m_OpenBLASModule.get_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m get_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenblas_get_config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    644\u001b[0m                      \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    645\u001b[0m get_config\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[0;32m--> 646\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenBLAS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced = tsne.fit_transform(X_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_a, cmap=\"jet\")\n",
    "plt.axis(\"off\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
